{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Google Colab venv setup\n",
    "!apt install python3.10-venv\n",
    "!python -m venv /content/my_venv\n",
    "!source /content/my_venv/bin/activate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Gemma RAG LLM setup**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installing the required packages\n",
    "!pip install langchain pandas==2.1.4 numpy==1.23.5 scipy==1.13.0 pymongo gradio requests langchain_community langchain_core langchain_mongodb sentence_transformers transformers python-dotenv tensorflow==2.15\n",
    "!pip install -U transformers\n",
    "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
    "#install below if using GPU\n",
    "!pip install accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the required libraries\n",
    "from pymongo import MongoClient\n",
    "from langchain_mongodb import MongoDBAtlasVectorSearch\n",
    "from langchain.document_loaders import DirectoryLoader\n",
    "from langchain.chains import RetrievalQA\n",
    "import gradio as gr\n",
    "from gradio.themes.base import Base\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnableParallel, RunnablePassthrough\n",
    "from sentence_transformers import SentenceTransformer # https://huggingface.co/thenlper/gte-large\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from transformers import AutoConfig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Accessing secrets***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In Google Colab, you can use the following code to access the secret\n",
    "from google.colab import userdata\n",
    "HF_Token = userdata.get('HF_TOKEN')\n",
    "MONGO_URI_SQL = userdata.get(\"MONGO_URI_SQL\")\n",
    "MONGO_URI_schema = userdata.get('MONGO_URI_Schema')\n",
    "\n",
    "# In your local environment, you can use the following code to access the secret\n",
    "#load_dotenv()\n",
    "#HF_Token = os.getenv(\"HF_Token\")\n",
    "#MONGO_URI_SQL = os.getenv(\"MONGO_URI_SQL\")\n",
    "#MONGO_URI_schema = os.getenv('MONGO_URI_schema')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Generating the embedding***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embedding model setup\n",
    "embedding_model = SentenceTransformer(\"thenlper/gte-large\")\n",
    "\n",
    "class CustomEmbeddingFunction:\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "\n",
    "    def embed_documents(self, texts):\n",
    "        \"\"\"Embeds a list of documents.\"\"\"\n",
    "        embeddings = self.model.encode(texts)\n",
    "        return embeddings.tolist()\n",
    "\n",
    "    def embed_query(self, text):\n",
    "        \"\"\"Embeds a single query.\"\"\"\n",
    "        embedding = self.model.encode(text)\n",
    "        return embedding.tolist()\n",
    "\n",
    "# Wrap the SentenceTransformer model\n",
    "embedding_function = CustomEmbeddingFunction(embedding_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## MongoDB setup\n",
    "# SQL Vector\n",
    "client_SQL = MongoClient(MONGO_URI_SQL)\n",
    "dbName_SQL = \"MVector\"\n",
    "collectionName_SQL = \"MTSQL\"\n",
    "collection_SQL = client_SQL[dbName_SQL][collectionName_SQL]\n",
    "index_name_SQL = \"vector_index_SQL\"\n",
    "\n",
    "## SQL Vector setup\n",
    "# Vector store setup\n",
    "vector_store_SQL = MongoDBAtlasVectorSearch(\n",
    "    client=client_SQL,\n",
    "    database=dbName_SQL,\n",
    "    collection=collection_SQL,\n",
    "    index_name=index_name_SQL,\n",
    "    embedding=embedding_function,\n",
    "    text_key=\"Query\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Schema Vector\n",
    "client_schema = MongoClient(MONGO_URI_schema)\n",
    "dbName_schema = \"MVector\"\n",
    "collectionName_schema = \"MTSchema\"\n",
    "collection_schema = client_schema[dbName_schema][collectionName_schema]\n",
    "index_name_schema = \"vector_index_schema\"\n",
    "\n",
    "## Schema Vector setup\n",
    "# Vector store setup\n",
    "vector_store_schema = MongoDBAtlasVectorSearch(\n",
    "    client=client_schema,\n",
    "    database=dbName_schema,\n",
    "    collection=collection_schema,\n",
    "    index_name=index_name_schema,\n",
    "    embedding=embedding_function,\n",
    "    text_key=\"Table_name\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Loading the Tokenizer and LLM-Model***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-7b-it\")\n",
    "# CPU Enabled uncomment below 👇🏽\n",
    "# model = AutoModelForCausalLM.from_pretrained(\"google/gemma-7b-it\")\n",
    "# GPU Enabled use below 👇🏽\n",
    "model = AutoModelForCausalLM.from_pretrained(\"google/gemma-7b-it\", device_map=\"auto\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Chain setup***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"SELECT T2.name ,  T2.capacity FROM concert AS T1 JOIN stadium AS T2 ON T1.stadium_id  =  T2.stadium_id WHERE T1.year  >=  2014 GROUP BY T2.stadium_id ORDER BY count(*) DESC LIMIT 1?\"\n",
    "\n",
    "# SQL Vector setup\n",
    "retriever_SQL = vector_store_SQL.as_retriever(search_kwargs={\"k\": 4})\n",
    "\n",
    "def logging_retriever_function_SQL(retriever_SQL, question):\n",
    "    documents_SQL = retriever_SQL.invoke(question)\n",
    "    print(\"Retrieved Documents:\")\n",
    "    for doc in documents_SQL:\n",
    "        print(doc)\n",
    "    return documents_SQL\n",
    "\n",
    "def get_source_information_SQL(question):\n",
    "    retrieved_docs = logging_retriever_function_SQL(retriever_SQL, question)\n",
    "    source_information_SQL = \"\\n\".join([str(doc) for doc in retrieved_docs])\n",
    "    return source_information_SQL\n",
    "\n",
    "information_summary_SQL = get_source_information_SQL(question)\n",
    "\n",
    "# Schema Vector setup\n",
    "retriever_schema = vector_store_schema.as_retriever(search_kwargs={\"k\": 10})\n",
    "\n",
    "def logging_retriever_function_schema(retriever_schema, question):\n",
    "    documents_schema = retriever_schema.invoke(question)\n",
    "    print(\"Retrieved Schema:\")\n",
    "    for doc in documents_schema:\n",
    "        print(doc)\n",
    "    return documents_schema\n",
    "\n",
    "def get_source_information_schema(question):\n",
    "    retrieved_docs = logging_retriever_function_schema(retriever_schema, question)\n",
    "    source_information_schema = \"\\n\".join([str(doc) for doc in retrieved_docs])\n",
    "    return source_information_schema\n",
    "\n",
    "information_summary_schema = get_source_information_schema(question)\n",
    "\n",
    "def generate_response(question):\n",
    "    combined_information = (\n",
    "          f\"Instructions: You are teaching SQL and need to help a student understand a given SQL statement. Translate the SQL query into natural language and explain how it works step by step. Use the examples provided in the Context string to guide your translation. Refer to the Schema string to understand the tables and columns in the database. If you can't answer the question, reply I don't know.\\n\\n\"\n",
    "          f\"Question: {question}\\n\\n\"\n",
    "          f\"Context: {information_summary_SQL}\\n\\n\"\n",
    "          f\"Schema: {information_summary_schema}\\n\\n\"\n",
    "          f\"Response:\\n\"\n",
    "    )\n",
    "\n",
    "    # Moving tensors to GPU\n",
    "    input_ids = tokenizer(combined_information, return_tensors=\"pt\").to(\"cuda\")\n",
    "    response = model.generate(**input_ids, max_new_tokens=1000)\n",
    "    decoded_response = tokenizer.decode(response[0], skip_special_tokens=True).strip()\n",
    "\n",
    "    # Post-processing: Extracting the content after 'Response:\\n'\n",
    "    if \"Response:\" in decoded_response:\n",
    "        decoded_response = decoded_response.split(\"Response:\", 1)[-1].strip()\n",
    "\n",
    "    return decoded_response\n",
    "\n",
    "# Example usage\n",
    "result = generate_response(question)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Chat interface setup***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the chain_invoke function\n",
    "def chain_invoke(question):\n",
    "    # Execute the chain with the logging retriever\n",
    "    result = generate_response(question)\n",
    "    # Return the result\n",
    "    return result\n",
    "\n",
    "# Create a web interface for the app, using Gradio\n",
    "with gr.Blocks(theme=Base(), title=\"Question Answering App using Vector Search + RAG\") as demo:\n",
    "    gr.Markdown(\n",
    "        \"\"\"\n",
    "        # Question Answering App using Atlas Vector Search + RAG Architecture\n",
    "        \"\"\")\n",
    "    textbox = gr.Textbox(label=\"Enter your SQL statement:\")\n",
    "    with gr.Row():\n",
    "        button = gr.Button(\"Submit\", variant=\"primary\")\n",
    "    with gr.Column():\n",
    "        output = gr.Textbox(lines=1, max_lines=30, label=\"Natural language translation and explanation:\")\n",
    "\n",
    "# Call chain_invoke function upon clicking the Submit button\n",
    "\n",
    "    button.click(chain_invoke, textbox, outputs=output)\n",
    "\n",
    "demo.launch()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
