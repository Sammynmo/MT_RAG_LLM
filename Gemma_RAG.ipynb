{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Gemma RAG LLM setup**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installing the required packages\n",
    "%pip3 install langchain pymongo gradio requests langchain_community langchain_core langchain_mongodb sentence_transformers transformers python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the required libraries\n",
    "from pymongo import MongoClient\n",
    "from langchain_mongodb import MongoDBAtlasVectorSearch\n",
    "from langchain.document_loaders import DirectoryLoader\n",
    "from langchain.chains import RetrievalQA\n",
    "import gradio as gr\n",
    "from gradio.themes.base import Base\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnableParallel, RunnablePassthrough\n",
    "from sentence_transformers import SentenceTransformer # https://huggingface.co/thenlper/gte-large\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import os\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Accessing secrets***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In Google Colab, you can use the following code to access the secret\n",
    "#from google.colab import userdata\n",
    "#HF_Token = userdata.get('HF_Token')\n",
    "MONGO_URI = os.getenv(\"MONGO_URI\")\n",
    "\n",
    "# In your local environment, you can use the following code to access the secret\n",
    "load_dotenv()\n",
    "HF_Token = os.getenv(\"HF_Token\")\n",
    "MONGO_URI = os.getenv(\"MONGO_URI\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Generating the embedding***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MongoDB setup\n",
    "client = MongoClient(MONGO_URI)\n",
    "dbName = \"MTGemma\"\n",
    "collectionName = \"MTGemma\"\n",
    "collection = client[dbName][collectionName]\n",
    "index_name = \"vector_index\"\n",
    "\n",
    "# Embedding model setup\n",
    "embedding_model = SentenceTransformer(\"thenlper/gte-large\")\n",
    "\n",
    "class CustomEmbeddingFunction:\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "\n",
    "    def embed_documents(self, texts):\n",
    "        \"\"\"Embeds a list of documents.\"\"\"\n",
    "        embeddings = self.model.encode(texts)\n",
    "        return embeddings.tolist()\n",
    "\n",
    "    def embed_query(self, text):\n",
    "        \"\"\"Embeds a single query.\"\"\"\n",
    "        embedding = self.model.encode(text)\n",
    "        return embedding.tolist()\n",
    "\n",
    "# Wrap the SentenceTransformer model\n",
    "embedding_function = CustomEmbeddingFunction(embedding_model)\n",
    "\n",
    "# Vector store setup\n",
    "vector_store = MongoDBAtlasVectorSearch(\n",
    "    client=client,\n",
    "    database=dbName,\n",
    "    collection=collection,\n",
    "    index_name=index_name,\n",
    "    embedding=embedding_function,\n",
    "    text_key=\"Query\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Loading the Tokenizer and LLM-Model***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-7b-it\")\n",
    "# CPU Enabled uncomment below 👇🏽\n",
    "# model = AutoModelForCausalLM.from_pretrained(\"google/gemma-7b-it\")\n",
    "# GPU Enabled use below 👇🏽\n",
    "model = AutoModelForCausalLM.from_pretrained(\"google/gemma-7b-it\", device_map=\"auto\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Chain setup***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"SELECT T2.name ,  T2.capacity FROM concert AS T1 JOIN stadium AS T2 ON T1.stadium_id  =  T2.stadium_id WHERE T1.year  >=  2014 GROUP BY T2.stadium_id ORDER BY count(*) DESC LIMIT 1?\"\n",
    "\n",
    "retriever = vector_store.as_retriever(search_kwargs={\"k\": 4})\n",
    "\n",
    "def logging_retriever_function(retriever, question):\n",
    "    documents = retriever.invoke(question)\n",
    "    print(\"Retrieved Documents:\")\n",
    "    for doc in documents:\n",
    "        print(doc)\n",
    "    return documents\n",
    "\n",
    "def get_source_information(question):\n",
    "    retrieved_docs = logging_retriever_function(retriever, question)\n",
    "    # Assuming doc itself contains what you need\n",
    "    source_information = \"\\n\".join([str(doc) for doc in retrieved_docs])  # If doc is a string or convertible to string\n",
    "    # Now use source_information as needed...\n",
    "    return source_information\n",
    "\n",
    "information_summary = get_source_information(question)\n",
    "\n",
    "def generate_response(question):\n",
    "    combined_information = (\n",
    "          f\"Instructions: Provide a natural language *translation* stating what the SQL statement achieves and *explanation* of the SQL code provided in the Question string. The Context string gives you examples of similar queries, you can use them to support your translation and explanation. Go through it step by step and if you can't answer the question, reply I don't know.\\n\\n\"\n",
    "          f\"Question: {question}\\n\\n\"\n",
    "          f\"Context: {information_summary}\\n\\n\"\n",
    "          f\"Response:\\n\"\n",
    "    )\n",
    "    # Moving tensors to GPU\n",
    "    input_ids = tokenizer(combined_information, return_tensors=\"pt\").to(\"cuda\")\n",
    "    response = model.generate(**input_ids, max_new_tokens=1000)\n",
    "    decoded_response = tokenizer.decode(response[0], skip_special_tokens=True).strip()\n",
    "\n",
    "    # Post-processing: Extracting the content after 'Response:\\n'\n",
    "    if \"Response:\" in decoded_response:\n",
    "        decoded_response = decoded_response.split(\"Response:\", 1)[-1].strip()\n",
    "\n",
    "    return decoded_response\n",
    "\n",
    "# Example usage\n",
    "result = generate_response(question)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Chat interface setup***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the chain_invoke function\n",
    "def chain_invoke(question):\n",
    "    # Execute the chain with the logging retriever\n",
    "    result = generate_response(question)\n",
    "    # Return the result\n",
    "    return result\n",
    "\n",
    "# Create a web interface for the app, using Gradio\n",
    "with gr.Blocks(theme=Base(), title=\"Question Answering App using Vector Search + RAG\") as demo:\n",
    "    gr.Markdown(\n",
    "        \"\"\"\n",
    "        # Question Answering App using Atlas Vector Search + RAG Architecture\n",
    "        \"\"\")\n",
    "    textbox = gr.Textbox(label=\"Enter your SQL statement:\")\n",
    "    with gr.Row():\n",
    "        button = gr.Button(\"Submit\", variant=\"primary\")\n",
    "    with gr.Column():\n",
    "        output = gr.Textbox(lines=1, max_lines=30, label=\"Natural language translation and explanation:\")\n",
    "\n",
    "# Call chain_invoke function upon clicking the Submit button\n",
    "\n",
    "    button.click(chain_invoke, textbox, outputs=output)\n",
    "\n",
    "demo.launch()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
