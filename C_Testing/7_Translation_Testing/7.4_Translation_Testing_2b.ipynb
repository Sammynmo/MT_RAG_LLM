{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\samir\\Documents\\GitHub\\MT_RAG_LLM\\mtgitenv\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Importing the required libraries\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sentence_transformers.util import cos_sim\n",
    "from sentence_transformers.quantization import quantize_embeddings\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accessing the secrets from the environment variables\n",
    "load_dotenv()\n",
    "HF_Token = os.getenv(\"HF_TOKEN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Path: ../8_Testing_Input_and_Output/App_Output_2b.csv\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3338ba70a3548ee83a471eafc641254",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            DB_ID                                              Query  \\\n",
      "0  concert_singer  SELECT T2.name ,  T2.capacity FROM concert AS ...   \n",
      "1          pets_1  SELECT T1.fname ,  T1.age FROM student AS T1 J...   \n",
      "2           car_1  SELECT T1.CountryName FROM COUNTRIES AS T1 JOI...   \n",
      "3           car_1  SELECT T2.MakeId ,  T2.Make FROM CARS_DATA AS ...   \n",
      "4           car_1  select t1.id ,  t1.maker from car_makers as t1...   \n",
      "\n",
      "                                            Question  \\\n",
      "0  Show the stadium name and capacity with most n...   \n",
      "1  Find the first name and age of students who ha...   \n",
      "2  Which countries in europe have at least 3 car ...   \n",
      "3  Among the cars with more than lowest horsepowe...   \n",
      "4  Which are the car makers which produce at leas...   \n",
      "\n",
      "                                              Output  \\\n",
      "0  **Translation:**\\nSelect the name and capacity...   \n",
      "1  **Translation:**\\nSelect the first name and ag...   \n",
      "2  **Translation:**  \\nSelect the names of countr...   \n",
      "3  **Translation:**\\nRetrieve the MakeId and Make...   \n",
      "4  **Translation:**\\nSelect the ID and maker of c...   \n",
      "\n",
      "                                         Translation  \\\n",
      "0  **Translation:**\\nSelect the name and capacity...   \n",
      "1  **Translation:**\\nSelect the first name and ag...   \n",
      "2  **Translation:**  \\nSelect the names of countr...   \n",
      "3  **Translation:**\\nRetrieve the MakeId and Make...   \n",
      "4  **Translation:**\\nSelect the ID and maker of c...   \n",
      "\n",
      "                                         Explanation  \n",
      "0  :**\\n1. **SELECT T2.name, T2.capacity**: This ...  \n",
      "1  :**\\n1. **SELECT T1.fname, T1.age**: This part...  \n",
      "2  of the SQL Query:**\\n\\n1. **SELECT T1.CountryN...  \n",
      "3  :**\\n1. **SELECT T2.MakeId, T2.Make**: This pa...  \n",
      "4  :**\\n1. **First Part of the Query:**\\n   - `SE...  \n"
     ]
    }
   ],
   "source": [
    "# Upload the dataset and transform to dataframe\n",
    "# Define the dataset path\n",
    "dataset_path = \"../8_Testing_Input_and_Output/App_Output_2b.csv\"\n",
    "print(\"Dataset Path:\", dataset_path)\n",
    "\n",
    "# Check if the file exists at the specified path\n",
    "if not os.path.isfile(dataset_path):\n",
    "    raise FileNotFoundError(f\"Unable to find the file at {dataset_path}\")\n",
    "\n",
    "# Load the dataset\n",
    "testing_output_2b = load_dataset('csv', data_files=dataset_path)\n",
    "\n",
    "# Convert the dataset to a pandas dataframe\n",
    "df_2b_testing_output = testing_output_2b['train'].to_pandas()\n",
    "\n",
    "# Print a few rows to verify\n",
    "print(df_2b_testing_output.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Version 1**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a pre-trained model for generating sentence embeddings\n",
    "embedding_model_1 = AutoModel.from_pretrained('jinaai/jina-embeddings-v3', trust_remote_code=True) # https://huggingface.co/jinaai/jina-embeddings-v3\n",
    "\n",
    "# Function to compute embeddings and similarity\n",
    "def Translation_assessment_1(df_2b_testing_output):\n",
    "    df_2b_testing_output['Question'] = df_2b_testing_output['Question'].fillna('').astype(str)\n",
    "    df_2b_testing_output['Translation'] = df_2b_testing_output['Translation'].fillna('').astype(str)\n",
    "    \n",
    "    # Generate embeddings for the \"Question\" and \"Translation\" columns\n",
    "    question_embeddings = embedding_model_1.encode(df_2b_testing_output['Question'].tolist(), task=\"text-matching\", convert_to_tensor=True)\n",
    "    translation_embeddings = embedding_model_1.encode(df_2b_testing_output['Translation'].tolist(), task=\"text-matching\", convert_to_tensor=True)\n",
    "\n",
    "    # Calculate cosine similarity for each row\n",
    "    similarities = cosine_similarity(question_embeddings, translation_embeddings)\n",
    "\n",
    "    # Since cosine_similarity returns a matrix, we extract the diagonal (row-wise comparison)\n",
    "    df_2b_testing_output['Similarity_1'] = np.diagonal(similarities)\n",
    "\n",
    "    return df_2b_testing_output\n",
    "\n",
    "# Call the function and process the dataframe\n",
    "df_translation_assessment_1 = Translation_assessment_1(df_2b_testing_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Version 2**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Specify preferred dimensions\n",
    "dimensions = 512\n",
    "\n",
    "# 2. Load the model\n",
    "embedding_model_2 = SentenceTransformer(\"mixedbread-ai/mxbai-embed-large-v1\", truncate_dim=dimensions)  # https://huggingface.co/mixedbread-ai/mxbai-embed-large-v1\n",
    "\n",
    "# Function to generate a detailed instruction for the query\n",
    "def get_detailed_instruct(task_description: str, query: str) -> str:\n",
    "    return f'Instruct: {task_description}\\nQuery: {query}'\n",
    "\n",
    "# Function to compute embeddings and similarity\n",
    "def Translation_assessment_2(df_2b_testing_output):\n",
    "    # Define task instruction for the queries\n",
    "    task = 'Compare the question and translation to assess the quality of the translation.'\n",
    "\n",
    "    # Add instruction to the \"Question\" column\n",
    "    questions_with_instructions = [\n",
    "        get_detailed_instruct(task, question) for question in df_2b_testing_output['Question'].tolist()\n",
    "    ]\n",
    "\n",
    "    # Generate a list of documents to encode\n",
    "    docs = questions_with_instructions + df_2b_testing_output['Translation'].tolist()\n",
    "    \n",
    "    # 2. Encode\n",
    "    embeddings = embedding_model_2.encode(docs)\n",
    "\n",
    "    # Optional: Quantize the embeddings\n",
    "    binary_embeddings = quantize_embeddings(embeddings, precision=\"ubinary\")\n",
    "\n",
    "    # Calculate cosine similarity between the first half (questions) and the second half (translations)\n",
    "    question_embeddings = embeddings[:len(questions_with_instructions)]\n",
    "    translation_embeddings = embeddings[len(questions_with_instructions):]\n",
    "\n",
    "    # Calculate cosine similarity\n",
    "    similarities = cos_sim(question_embeddings, translation_embeddings)\n",
    "\n",
    "    # Since cos_sim returns a matrix, we extract the diagonal (row-wise comparison)\n",
    "    df_2b_testing_output['Similarity_V2'] = np.diagonal(similarities.cpu().numpy())\n",
    "\n",
    "    return df_2b_testing_output\n",
    "\n",
    "# Call the function and process the dataframe\n",
    "df_translation_assessment_2 = Translation_assessment_2(df_2b_testing_output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Version 3**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a pre-trained model for generating sentence embeddings\n",
    "embedding_model = SentenceTransformer(\"thenlper/gte-large\")\n",
    "\n",
    "# Function to compute embeddings and similarity\n",
    "def Translation_assessment_3(df_2b_testing_output):\n",
    "    # Generate embeddings for the \"Question\" and \"Translation\" columns\n",
    "    question_embeddings = embedding_model.encode(df_2b_testing_output['Question'].tolist(), convert_to_tensor=True)\n",
    "    translation_embeddings = embedding_model.encode(df_2b_testing_output['Translation'].tolist(), convert_to_tensor=True)\n",
    "\n",
    "    # Calculate cosine similarity for each row\n",
    "    similarities = cosine_similarity(question_embeddings, translation_embeddings)\n",
    "\n",
    "    # Since cosine_similarity returns a matrix, we extract the diagonal (row-wise comparison)\n",
    "    df_2b_testing_output['Similarity_V3'] = np.diagonal(similarities)\n",
    "\n",
    "    return df_2b_testing_output\n",
    "\n",
    "# Call the function and process the dataframe\n",
    "df_translation_assessment_3 = Translation_assessment_3(df_2b_testing_output)\n",
    "\n",
    "# Saving to CSV with the similarity score\n",
    "df_translation_assessment_3.to_csv('../8_Testing_Input_and_Output/Translation_assessment_2b.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mtgitenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
