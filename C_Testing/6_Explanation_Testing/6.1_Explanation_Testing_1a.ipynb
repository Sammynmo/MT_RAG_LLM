{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Testing of 1a Gemma with RAG and Schema***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Loading packages, libraries and secrets into notebook**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the required libraries\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_openai import OpenAI\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnableParallel, RunnablePassthrough\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_anthropic import ChatAnthropic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accessing the secrets from the environment variables\n",
    "load_dotenv()\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "GOOGLE_API_KEY = os.getenv(\"GOOGLE_API_KEY\")\n",
    "ANTHROPIC_API_KEY = os.getenv(\"ANTHROPIC_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Loading data into dataframe for testing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Path: ../8_Testing_Input_and_Output/App_Output_1a.csv\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "336f9e5ffad64eee8e5aebc2ac2d79e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            DB_ID                                              Query  \\\n",
      "0  concert_singer  SELECT T2.name ,  T2.capacity FROM concert AS ...   \n",
      "1          pets_1  SELECT T1.fname ,  T1.age FROM student AS T1 J...   \n",
      "2           car_1  SELECT T1.CountryName FROM COUNTRIES AS T1 JOI...   \n",
      "3           car_1  SELECT T2.MakeId ,  T2.Make FROM CARS_DATA AS ...   \n",
      "4           car_1  select t1.id ,  t1.maker from car_makers as t1...   \n",
      "\n",
      "                                            Question  \\\n",
      "0  Show the stadium name and capacity with most n...   \n",
      "1  Find the first name and age of students who ha...   \n",
      "2  Which countries in europe have at least 3 car ...   \n",
      "3  Among the cars with more than lowest horsepowe...   \n",
      "4  Which are the car makers which produce at leas...   \n",
      "\n",
      "                                              Output  \\\n",
      "0  The query aims to find the name of the stadium...   \n",
      "1  The query aims to find the first name and age ...   \n",
      "2  The query aims to identify the countries that ...   \n",
      "3  The query aims to find the project that requir...   \n",
      "4  Sure, here is the translation and explanation ...   \n",
      "\n",
      "                                         Translation  \\\n",
      "0  The query wants to find the name of the stadiu...   \n",
      "1  The query aims to find the first name and age ...   \n",
      "2  The query aims to identify the countries that ...   \n",
      "3  The query aims to find the project that requir...   \n",
      "4  The query aims to identify assets that have tw...   \n",
      "\n",
      "                                         Explanation  \n",
      "0  The query aims to find the name of the stadium...  \n",
      "1  1. **Selecting Columns:**\\n   - The query sele...  \n",
      "2  1. **Selecting T1.CountryName:** The query sel...  \n",
      "3  The query is composed of several parts:\\n\\n- *...  \n",
      "4  The query is composed of several clauses and j...  \n"
     ]
    }
   ],
   "source": [
    "# Upload the dataset and transform to dataframe\n",
    "# Define the dataset path\n",
    "dataset_path = \"../8_Testing_Input_and_Output/App_Output_1a.csv\"\n",
    "print(\"Dataset Path:\", dataset_path)\n",
    "\n",
    "# Check if the file exists at the specified path\n",
    "if not os.path.isfile(dataset_path):\n",
    "    raise FileNotFoundError(f\"Unable to find the file at {dataset_path}\")\n",
    "\n",
    "# Load the dataset\n",
    "testing_output_1a = load_dataset('csv', data_files=dataset_path)\n",
    "\n",
    "# Convert the dataset to a pandas dataframe\n",
    "df_1a_testing_output = testing_output_1a['train'].to_pandas()\n",
    "\n",
    "# Print a few rows to verify\n",
    "print(df_1a_testing_output.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Testing Template**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chain setup (the model chain for comparison)\n",
    "testing_template = \"\"\"\n",
    "\"How well does the following Explanation explain the SQL Query? Please assess it critically then assign and output one of the following scores where 4 is the highest and 1 is the lowest: Acceptable (4), Minor errors (3), Major errors (2), or Unacceptable (1). To determine the score, go through the assessment step by step and consider the accuracy and understandability of the translation and explanation.\"\n",
    "\n",
    "SQL Query: {query}\n",
    "\n",
    "Explanation: {explanation}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "\n",
    "prompt_testing = ChatPromptTemplate.from_template(testing_template)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**OpenAI Assessment**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model and parsing setup\n",
    "model = ChatOpenAI(api_key=OPENAI_API_KEY, model=\"gpt-4o-mini\")\n",
    "parser = StrOutputParser()\n",
    "\n",
    "chain_testing_OAI = (\n",
    "    {\"query\": RunnablePassthrough(), \"explanation\": RunnablePassthrough(), \"question\": RunnablePassthrough()}\n",
    "    | prompt_testing\n",
    "    | model\n",
    "    | parser\n",
    ")\n",
    "\n",
    "# Function to compare each question and result using the chain\n",
    "def Explanation_testing_OAI(df_1a_testing_output):\n",
    "    assessment_OAI = []\n",
    "    \n",
    "    for i, row in df_1a_testing_output.iterrows():\n",
    "        # Get the question and result from the dataframe\n",
    "        query = row[\"Query\"]\n",
    "        question = row[\"Question\"]\n",
    "        explanation = row[\"Explanation\"]\n",
    "        \n",
    "        # Create a dictionary with query and result to pass to the chain\n",
    "        inputs = {\"query\": query, \"explanation\": explanation, \"question\" : question}\n",
    "\n",
    "        # Run the chain and catch any potential errors\n",
    "        try:\n",
    "            test_output_OAI = chain_testing_OAI.invoke(inputs)\n",
    "        except Exception as e:\n",
    "            test_output_OAI = f\"Error in row {i}: {str(e)}\"\n",
    "        \n",
    "        # Store the comparison output\n",
    "        assessment_OAI.append( test_output_OAI)\n",
    "    \n",
    "    # Add the comparison results to a new column\n",
    "    df_1a_testing_output[\"Assessment OAI\"] = assessment_OAI\n",
    "    \n",
    "    return df_1a_testing_output\n",
    "\n",
    "# Call the function and process the dataframe\n",
    "df_explanation_assessment_OAI = Explanation_testing_OAI(df_1a_testing_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Gemini Assessment**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "Gemini_model = ChatGoogleGenerativeAI(model=\"gemini-pro\", api_key=GOOGLE_API_KEY)\n",
    "\n",
    "\n",
    "chain_testing_Gemi = (\n",
    "    {\"query\": RunnablePassthrough(), \"explanation\": RunnablePassthrough(), \"question\": RunnablePassthrough()}\n",
    "    | prompt_testing\n",
    "    | Gemini_model\n",
    "    | parser\n",
    ")\n",
    "\n",
    "# Function to compare each question and result using the chain\n",
    "def testing_Gemi(df_1a_testing_output):\n",
    "    assessment_Gemi = []\n",
    "    \n",
    "    for i, row in df_1a_testing_output.iterrows():\n",
    "        # Get the question and result from the dataframe\n",
    "        query = row[\"Query\"]\n",
    "        question = row[\"Question\"]\n",
    "        explanation = row[\"Explanation\"]\n",
    "        \n",
    "        # Create a dictionary with query and result to pass to the chain\n",
    "        inputs = {\"query\": query, \"explanation\": explanation, \"question\" : question}\n",
    "\n",
    "        # Run the chain and catch any potential errors\n",
    "        try:\n",
    "            test_output_Gemi = chain_testing_Gemi.invoke(inputs)\n",
    "        except Exception as e:\n",
    "            test_output_Gemi = f\"Error in row {i}: {str(e)}\"\n",
    "        \n",
    "        # Store the comparison output\n",
    "        assessment_Gemi.append(test_output_Gemi)\n",
    "    \n",
    "    # Add the comparison results to a new column\n",
    "    df_1a_testing_output[\"Assessment Gemini\"] = assessment_Gemi\n",
    "    \n",
    "    return df_1a_testing_output\n",
    "\n",
    "# Call the function and process the dataframe\n",
    "df_explanation_assessment_Gemi = testing_Gemi(df_1a_testing_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Claude Assessment**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "Claude_model = ChatAnthropic(model=\"claude-3-5-sonnet-20240620\", api_key=ANTHROPIC_API_KEY)\n",
    "\n",
    "\n",
    "chain_testing_Claude = (\n",
    "    {\"query\": RunnablePassthrough(), \"explanation\": RunnablePassthrough(), \"question\": RunnablePassthrough()}\n",
    "    | prompt_testing\n",
    "    | Claude_model\n",
    "    | parser\n",
    ")\n",
    "\n",
    "# Function to compare each question and result using the chain\n",
    "def testing_Claude(df_1a_testing_output):\n",
    "    assessment_Claude = []\n",
    "    \n",
    "    for i, row in df_1a_testing_output.iterrows():\n",
    "        # Get the question and result from the dataframe\n",
    "        query = row[\"Query\"]\n",
    "        question = row[\"Question\"]\n",
    "        explanation = row[\"Explanation\"]\n",
    "        \n",
    "        # Create a dictionary with query and result to pass to the chain\n",
    "        inputs = {\"query\": query, \"explanation\": explanation, \"question\" : question}\n",
    "\n",
    "        # Run the chain and catch any potential errors\n",
    "        try:\n",
    "            test_output_Claude = chain_testing_Claude.invoke(inputs)\n",
    "        except Exception as e:\n",
    "            test_output_Claude = f\"Error in row {i}: {str(e)}\"\n",
    "        \n",
    "        # Store the comparison output\n",
    "        assessment_Claude.append(test_output_Claude)\n",
    "    \n",
    "    # Add the comparison results to a new column\n",
    "    df_1a_testing_output[\"Assessment Claude\"] = assessment_Claude\n",
    "    \n",
    "    return df_1a_testing_output\n",
    "\n",
    "# Call the function and process the dataframe\n",
    "df_explanation_assessment_Claude = testing_Claude(df_1a_testing_output)\n",
    "\n",
    "# Save the dataframe, including the comparison, to a CSV file\n",
    "df_explanation_assessment_Claude.to_csv(\"../8_Testing_Input_and_Output/Explanation_assessment_1a.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mtgitenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
