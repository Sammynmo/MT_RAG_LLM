{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***OpenAI App Testing run and generation of file with App Output***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Loading packages, libraries and secrets into notebook**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the required libraries\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from datasets import load_dataset\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accessing the secrets from the environment variables\n",
    "load_dotenv()\n",
    "MONGO_URI_SQL = os.getenv(\"MONGO_URI_SQL\")\n",
    "MONGO_URI_schema = os.getenv(\"MONGO_URI_Schema\")\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "HF_Token = os.getenv(\"HF_TOKEN\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Testing App 1b**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ../../A_Apps/1b_Openai_RAG_Schema.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload the dataset and transform to dataframe\n",
    "# Define the dataset path\n",
    "dataset_path = \"../8_Testing_Input_and_Output/Spider_Testing_Selection.csv\"\n",
    "print(\"Dataset Path:\", dataset_path)\n",
    "\n",
    "# Check if the file exists at the specified path\n",
    "if not os.path.isfile(dataset_path):\n",
    "    raise FileNotFoundError(f\"Unable to find the file at {dataset_path}\")\n",
    "\n",
    "# Load the dataset\n",
    "testing_1b = load_dataset('csv', data_files=dataset_path)\n",
    "\n",
    "# Convert the dataset to a pandas dataframe\n",
    "df_1b_testing = testing_1b[\"train\"].to_pandas()\n",
    "\n",
    "# Print a few rows to verify\n",
    "print(df_1b_testing.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to run the chain for each query\n",
    "def process_queries(df_1b_testing):\n",
    "    # Create an empty list to store the results\n",
    "    output = []\n",
    "\n",
    "    for i, row in df_1b_testing.iterrows():\n",
    "        # Get the query from the dataframe\n",
    "        query = row[\"Query\"]\n",
    "        DB_name = row[\"DB_name\"]\n",
    "        input_value = query if not DB_name else DB_name + query\n",
    "\n",
    "        # Execute the chain with the current query\n",
    "        try:\n",
    "            result = chain_1b.invoke(input_value)\n",
    "        except Exception as e:\n",
    "            result = f\"Error processing query {i}: {str(e)}\"\n",
    "        \n",
    "        # Append the result to the list\n",
    "        output.append(result)\n",
    "\n",
    "    # Add the results to a new column in the dataframe\n",
    "    df_1b_testing[\"Output\"] = output\n",
    "    \n",
    "# Check and split the Output column into two: Translation and Explanation\n",
    "    def split_output(text):\n",
    "        if 'Explanation' in text:\n",
    "            parts = text.split('Explanation', 1)\n",
    "            return parts[0].strip(), parts[1].strip()\n",
    "        else:\n",
    "            return text, None  # If no \"Explanation\", return the text as translation, and None for explanation\n",
    "\n",
    "    # Apply the splitting function to the Output column\n",
    "    df_1b_testing[['Translation', 'Explanation']] = df_1b_testing[\"Output\"].apply(lambda x: pd.Series(split_output(x)))\n",
    "    \n",
    "    return df_1b_testing\n",
    "\n",
    "# Call the function and process the dataframe\n",
    "df_1b_testing_output = process_queries(df_1b_testing)\n",
    "\n",
    "# Now 'df_with_results' contains the original queries and their corresponding results\n",
    "print(df_1b_testing_output)\n",
    "df_1b_testing_output.to_csv(\"../8_Testing_Input_and_Output/App_Output_1b.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Testing App 2b**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ../../A_Apps/2b_Openai_RAG.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload the dataset and transform to dataframe\n",
    "# Define the dataset path\n",
    "dataset_path = \"../8_Testing_Input_and_Output/Spider_Testing_Selection.csv\"\n",
    "print(\"Dataset Path:\", dataset_path)\n",
    "\n",
    "# Check if the file exists at the specified path\n",
    "if not os.path.isfile(dataset_path):\n",
    "    raise FileNotFoundError(f\"Unable to find the file at {dataset_path}\")\n",
    "\n",
    "# Load the dataset\n",
    "testing_2b = load_dataset('csv', data_files=dataset_path)\n",
    "\n",
    "# Convert the dataset to a pandas dataframe\n",
    "df_2b_testing = testing_2b[\"train\"].to_pandas()\n",
    "\n",
    "# Print a few rows to verify\n",
    "print(df_2b_testing.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to run the chain for each query\n",
    "def process_queries(df_2b_testing):\n",
    "    # Create an empty list to store the results\n",
    "    output = []\n",
    "\n",
    "    for i, row in df_2b_testing.iterrows():\n",
    "        # Get the query from the dataframe\n",
    "        query = row[\"Query\"]\n",
    "\n",
    "        # Execute the chain with the current query\n",
    "        try:\n",
    "            result = chain_2b.invoke(query)\n",
    "        except Exception as e:\n",
    "            result = f\"Error processing query {i}: {str(e)}\"\n",
    "        \n",
    "        # Append the result to the list\n",
    "        output.append(result)\n",
    "\n",
    "    # Add the results to a new column in the dataframe\n",
    "    df_2b_testing[\"Output\"] = output\n",
    "    \n",
    "# Check and split the Output column into two: Translation and Explanation\n",
    "    def split_output(text):\n",
    "        if 'Explanation' in text:\n",
    "            parts = text.split('Explanation', 1)\n",
    "            return parts[0].strip(), parts[1].strip()\n",
    "        else:\n",
    "            return text, None  # If no \"Explanation:\", return the text as translation, and None for explanation\n",
    "\n",
    "    # Apply the splitting function to the Output column\n",
    "    df_2b_testing[['Translation', 'Explanation']] = df_2b_testing[\"Output\"].apply(lambda x: pd.Series(split_output(x)))\n",
    "    \n",
    "    return df_2b_testing\n",
    "\n",
    "# Call the function and process the dataframe\n",
    "df_2b_testing_output = process_queries(df_2b_testing)\n",
    "\n",
    "# Now 'df_with_results' contains the original queries and their corresponding results\n",
    "print(df_2b_testing_output)\n",
    "df_2b_testing_output.to_csv(\"../8_Testing_Input_and_Output/App_Output_2b.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Testing App 3b**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ../../A_Apps/3b_Openai.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload the dataset and transform to dataframe\n",
    "# Define the dataset path\n",
    "dataset_path = \"../8_Testing_Input_and_Output/Spider_Testing_Selection.csv\"\n",
    "print(\"Dataset Path:\", dataset_path)\n",
    "\n",
    "# Check if the file exists at the specified path\n",
    "if not os.path.isfile(dataset_path):\n",
    "    raise FileNotFoundError(f\"Unable to find the file at {dataset_path}\")\n",
    "\n",
    "# Load the dataset\n",
    "testing_3b = load_dataset('csv', data_files=dataset_path)\n",
    "\n",
    "# Convert the dataset to a pandas dataframe\n",
    "df_3b_testing = testing_3b[\"train\"].to_pandas()\n",
    "\n",
    "# Print a few rows to verify\n",
    "print(df_3b_testing.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to run the chain for each query\n",
    "def process_queries(df_3b_testing):\n",
    "    # Create an empty list to store the results\n",
    "    output = []\n",
    "\n",
    "    for i, row in df_3b_testing.iterrows():\n",
    "        # Get the query from the dataframe\n",
    "        query = row[\"Query\"]\n",
    "\n",
    "        # Execute the chain with the current query\n",
    "        try:\n",
    "            result = chain_3b.invoke(query)\n",
    "        except Exception as e:\n",
    "            result = f\"Error processing query {i}: {str(e)}\"\n",
    "        \n",
    "        # Append the result to the list\n",
    "        output.append(result)\n",
    "\n",
    "    # Add the results to a new column in the dataframe\n",
    "    df_3b_testing[\"Output\"] = output\n",
    "    \n",
    "# Check and split the Output column into two: Translation and Explanation\n",
    "    def split_output(text):\n",
    "        if 'Explanation' in text:\n",
    "            parts = text.split('Explanation', 1)\n",
    "            return parts[0].strip(), parts[1].strip()\n",
    "        else:\n",
    "            return text, None  # If no \"Explanation:\", return the text as translation, and None for explanation\n",
    "\n",
    "    # Apply the splitting function to the Output column\n",
    "    df_3b_testing[['Translation', 'Explanation']] = df_3b_testing[\"Output\"].apply(lambda x: pd.Series(split_output(x)))\n",
    "    \n",
    "    return df_3b_testing\n",
    "\n",
    "# Call the function and process the dataframe\n",
    "df_3b_testing_output = process_queries(df_3b_testing)\n",
    "\n",
    "# Now 'df_with_results' contains the original queries and their corresponding results\n",
    "print(df_3b_testing_output)\n",
    "df_3b_testing_output.to_csv(\"../8_Testing_Input_and_Output/App_Output_3b.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mtgitenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
