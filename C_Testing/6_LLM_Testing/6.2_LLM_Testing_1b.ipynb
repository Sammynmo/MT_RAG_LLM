{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Testing of 1b Openai with RAG and Schema***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Loading packages, libraries and secrets into notebook**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the required libraries\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_openai import OpenAI\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnableParallel, RunnablePassthrough\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_anthropic import ChatAnthropic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accessing the secrets from the environment variables\n",
    "load_dotenv()\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "GOOGLE_API_KEY = os.getenv(\"GOOGLE_API_KEY\")\n",
    "ANTHROPIC_API_KEY = os.getenv(\"ANTHROPIC_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Loading data into dataframe for testing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Path: ../8_Testing_Input_and_Output/App_Output_1b.csv\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8d30a066c044890b0efeecdb9605cec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            DB_ID                                              Query  \\\n",
      "0  concert_singer  SELECT T2.name ,  T2.capacity FROM concert AS ...   \n",
      "1          pets_1  SELECT T1.fname ,  T1.age FROM student AS T1 J...   \n",
      "2           car_1  SELECT T1.CountryName FROM COUNTRIES AS T1 JOI...   \n",
      "3           car_1  SELECT T2.MakeId ,  T2.Make FROM CARS_DATA AS ...   \n",
      "4           car_1  select t1.id ,  t1.maker from car_makers as t1...   \n",
      "\n",
      "                                            Question  \\\n",
      "0  Show the stadium name and capacity with most n...   \n",
      "1  Find the first name and age of students who ha...   \n",
      "2  Which countries in europe have at least 3 car ...   \n",
      "3  Among the cars with more than lowest horsepowe...   \n",
      "4  Which are the car makers which produce at leas...   \n",
      "\n",
      "                                              Output  \\\n",
      "0  **Translation:**\\nRetrieve the name and capaci...   \n",
      "1  **Translation:**\\nSelect the first name and ag...   \n",
      "2  **Translation:**\\nSelect the names of countrie...   \n",
      "3  **Translation:**\\nSelect the MakeId and Make o...   \n",
      "4  **Translation:**\\nSelect the ID and maker of c...   \n",
      "\n",
      "                                         Translation  \\\n",
      "0  **Translation:**\\nRetrieve the name and capaci...   \n",
      "1  **Translation:**\\nSelect the first name and ag...   \n",
      "2  **Translation:**\\nSelect the names of countrie...   \n",
      "3  **Translation:**\\nSelect the MakeId and Make o...   \n",
      "4  **Translation:**\\nSelect the ID and maker of c...   \n",
      "\n",
      "                                         Explanation  \n",
      "0  of the SQL Query:**\\n\\n1. **SELECT T2.name, T2...  \n",
      "1  of the SQL Query:**\\n\\n1. **SELECT T1.fname, T...  \n",
      "2  :**\\n1. **SELECT T1.CountryName**: This part s...  \n",
      "3  :**\\n1. **SELECT T2.MakeId, T2.Make**: This pa...  \n",
      "4  :**\\n1. **First Part of the Query:**\\n   - `SE...  \n"
     ]
    }
   ],
   "source": [
    "# Upload the dataset and transform to dataframe\n",
    "# Define the dataset path\n",
    "dataset_path = \"../8_Testing_Input_and_Output/App_Output_1b.csv\"\n",
    "print(\"Dataset Path:\", dataset_path)\n",
    "\n",
    "# Check if the file exists at the specified path\n",
    "if not os.path.isfile(dataset_path):\n",
    "    raise FileNotFoundError(f\"Unable to find the file at {dataset_path}\")\n",
    "\n",
    "# Load the dataset\n",
    "testing_output_1b = load_dataset('csv', data_files=dataset_path)\n",
    "\n",
    "# Convert the dataset to a pandas dataframe\n",
    "df_1b_testing_output = testing_output_1b['train'].to_pandas()\n",
    "\n",
    "# Print a few rows to verify\n",
    "print(df_1b_testing_output.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Testing Template**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Explanation Assessment Template*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chain setup explanation\n",
    "testing_template_explanation = \"\"\"\n",
    "\"How well does the following Explanation explain the SQL Query? Please assess it critically then assign and output one of the following scores where 4 is the highest and 1 is the lowest: Acceptable (4), Minor errors (3), Major errors (2), or Unacceptable (1). To determine the score, go through the assessment step by step and consider the accuracy and understandability of the explanation assigning one score for accuracy and understandability, and a combined overall score for the explanation.\"\n",
    "\n",
    "SQL Query: {query}\n",
    "\n",
    "Explanation: {explanation}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "\n",
    "prompt_testing_explanation = ChatPromptTemplate.from_template(testing_template_explanation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Translation Assessment Template*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chain setup translation\n",
    "testing_template_translation = \"\"\"\n",
    "\"How well does the following Translation translate the SQL Query? Please assess it critically then assign and output one of the following scores where 4 is the highest and 1 is the lowest: Acceptable (4), Minor errors (3), Major errors (2), or Unacceptable (1). To determine the score, go through the assessment step by step and consider the accuracy and understandability of the translation assigning one score for accuracy and understandability, and a combined overall score for the translation.\"\n",
    "\n",
    "SQL Query: {query}\n",
    "\n",
    "Translation: {translation}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "\n",
    "prompt_testing_translation = ChatPromptTemplate.from_template(testing_template_translation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**OpenAI Assessment**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Explanation Assessment*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model and parsing setup\n",
    "model = ChatOpenAI(api_key=OPENAI_API_KEY, model=\"gpt-4o-mini\")\n",
    "parser = StrOutputParser()\n",
    "\n",
    "chain_testing_OAI_explanation = (\n",
    "    {\"query\": RunnablePassthrough(), \"explanation\": RunnablePassthrough(), \"question\": RunnablePassthrough()}\n",
    "    | prompt_testing_explanation\n",
    "    | model\n",
    "    | parser\n",
    ")\n",
    "\n",
    "# Function to compare each question and result using the chain\n",
    "def Explanation_testing_OAI(df_1b_testing_output):\n",
    "    assessment_OAI_explanation = []\n",
    "    \n",
    "    for i, row in df_1b_testing_output.iterrows():\n",
    "        # Get the question and result from the dataframe\n",
    "        query = row[\"Query\"]\n",
    "        question = row[\"Question\"]\n",
    "        explanation = row[\"Explanation\"]\n",
    "        \n",
    "        # Create a dictionary with query and result to pass to the chain\n",
    "        inputs = {\"query\": query, \"explanation\": explanation, \"question\" : question}\n",
    "\n",
    "        # Run the chain and catch any potential errors\n",
    "        try:\n",
    "            test_output_OAI_explanation = chain_testing_OAI_explanation.invoke(inputs)\n",
    "        except Exception as e:\n",
    "            test_output_OAI_explanation = f\"Error in row {i}: {str(e)}\"\n",
    "        \n",
    "        # Store the comparison output\n",
    "        assessment_OAI_explanation.append( test_output_OAI_explanation)\n",
    "    \n",
    "    # Add the comparison results to a new column\n",
    "    df_1b_testing_output[\"Assessment OAI Explanation\"] = assessment_OAI_explanation\n",
    "    \n",
    "    return df_1b_testing_output\n",
    "\n",
    "# Call the function and process the dataframe\n",
    "df_explanation_assessment_OAI_ = Explanation_testing_OAI(df_1b_testing_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Translation Assessment*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain_testing_OAI_translation = (\n",
    "    {\"query\": RunnablePassthrough(), \"translation\": RunnablePassthrough(), \"question\": RunnablePassthrough()}\n",
    "    | prompt_testing_translation\n",
    "    | model\n",
    "    | parser\n",
    ")\n",
    "\n",
    "# Function to compare each question and result using the chain\n",
    "def Translation_testing_OAI(df_1b_testing_output):\n",
    "    assessment_OAI_translation = []\n",
    "    \n",
    "    for i, row in df_1b_testing_output.iterrows():\n",
    "        # Get the question and result from the dataframe\n",
    "        query = row[\"Query\"]\n",
    "        translation = row[\"Translation\"]\n",
    "        question = row[\"Question\"]\n",
    "        \n",
    "        # Create a dictionary with query and result to pass to the chain\n",
    "        inputs = {\"query\": query, \"translation\": translation, \"question\" : question}\n",
    "\n",
    "        # Run the chain and catch any potential errors\n",
    "        try:\n",
    "            test_output_OAI_translation = chain_testing_OAI_translation.invoke(inputs)\n",
    "        except Exception as e:\n",
    "            test_output_OAI_translation = f\"Error in row {i}: {str(e)}\"\n",
    "        \n",
    "        # Store the comparison output\n",
    "        assessment_OAI_translation.append( test_output_OAI_translation)\n",
    "    \n",
    "    # Add the comparison results to a new column\n",
    "    df_1b_testing_output[\"Assessment OAI Translation\"] = assessment_OAI_translation\n",
    "    \n",
    "    return df_1b_testing_output\n",
    "\n",
    "# Call the function and process the dataframe\n",
    "df_translation_assessment_OAI = Translation_testing_OAI(df_1b_testing_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Gemini Assessment**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Explanation Assessment*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "Gemini_model = ChatGoogleGenerativeAI(model=\"gemini-pro\", api_key=GOOGLE_API_KEY)\n",
    "\n",
    "\n",
    "chain_testing_Gemi_explanation = (\n",
    "    {\"query\": RunnablePassthrough(), \"explanation\": RunnablePassthrough(), \"question\": RunnablePassthrough()}\n",
    "    | prompt_testing_explanation\n",
    "    | Gemini_model\n",
    "    | parser\n",
    ")\n",
    "\n",
    "# Function to compare each question and result using the chain\n",
    "def Explanation_testing_Gemi(df_1b_testing_output):\n",
    "    assessment_Gemi_explanation = []\n",
    "    \n",
    "    for i, row in df_1b_testing_output.iterrows():\n",
    "        # Get the question and result from the dataframe\n",
    "        query = row[\"Query\"]\n",
    "        question = row[\"Question\"]\n",
    "        explanation = row[\"Explanation\"]\n",
    "        \n",
    "        # Create a dictionary with query and result to pass to the chain\n",
    "        inputs = {\"query\": query, \"explanation\": explanation, \"question\" : question}\n",
    "\n",
    "        # Run the chain and catch any potential errors\n",
    "        try:\n",
    "            test_output_Gemi_explanation = chain_testing_Gemi_explanation.invoke(inputs)\n",
    "        except Exception as e:\n",
    "            test_output_Gemi_explanation = f\"Error in row {i}: {str(e)}\"\n",
    "        \n",
    "        # Store the comparison output\n",
    "        assessment_Gemi_explanation.append(test_output_Gemi_explanation)\n",
    "    \n",
    "    # Add the comparison results to a new column\n",
    "    df_1b_testing_output[\"Assessment Gemini Explanation\"] = assessment_Gemi_explanation\n",
    "    \n",
    "    return df_1b_testing_output\n",
    "\n",
    "# Call the function and process the dataframe\n",
    "df_explanation_assessment_Gemi = Explanation_testing_Gemi(df_1b_testing_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Translation Assessment*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain_testing_Gemi_translation = (\n",
    "    {\"query\": RunnablePassthrough(), \"translation\": RunnablePassthrough(), \"question\": RunnablePassthrough()}\n",
    "    | prompt_testing_translation\n",
    "    | Gemini_model\n",
    "    | parser\n",
    ")\n",
    "\n",
    "# Function to compare each question and result using the chain\n",
    "def Translation_testing_Gemi(df_1b_testing_output):\n",
    "    assessment_Gemi_translation = []\n",
    "    \n",
    "    for i, row in df_1b_testing_output.iterrows():\n",
    "        # Get the question and result from the dataframe\n",
    "        query = row[\"Query\"]\n",
    "        question = row[\"Question\"]\n",
    "        translation = row[\"Translation\"]\n",
    "        \n",
    "        # Create a dictionary with query and result to pass to the chain\n",
    "        inputs = {\"query\": query, \"translation\": translation, \"question\" : question}\n",
    "\n",
    "        # Run the chain and catch any potential errors\n",
    "        try:\n",
    "            test_output_Gemi_translation = chain_testing_Gemi_translation.invoke(inputs)\n",
    "        except Exception as e:\n",
    "            test_output_Gemi_translation = f\"Error in row {i}: {str(e)}\"\n",
    "        \n",
    "        # Store the comparison output\n",
    "        assessment_Gemi_translation.append(test_output_Gemi_translation)\n",
    "    \n",
    "    # Add the comparison results to a new column\n",
    "    df_1b_testing_output[\"Assessment Gemini Translation\"] = assessment_Gemi_translation\n",
    "    \n",
    "    return df_1b_testing_output\n",
    "\n",
    "# Call the function and process the dataframe\n",
    "df_translation_assessment_Gemi = Translation_testing_Gemi(df_1b_testing_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Claude Assessment**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Explanation Assessment*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "Claude_model = ChatAnthropic(model=\"claude-3-5-sonnet-20240620\", api_key=ANTHROPIC_API_KEY)\n",
    "\n",
    "\n",
    "chain_testing_Claude_explanation = (\n",
    "    {\"query\": RunnablePassthrough(), \"explanation\": RunnablePassthrough(), \"question\": RunnablePassthrough()}\n",
    "    | prompt_testing_explanation\n",
    "    | Claude_model\n",
    "    | parser\n",
    ")\n",
    "\n",
    "# Function to compare each question and result using the chain\n",
    "def Explanation_testing_Claude(df_1b_testing_output):\n",
    "    assessment_Claude_explanation = []\n",
    "    \n",
    "    for i, row in df_1b_testing_output.iterrows():\n",
    "        # Get the question and result from the dataframe\n",
    "        query = row[\"Query\"]\n",
    "        question = row[\"Question\"]\n",
    "        explanation = row[\"Explanation\"]\n",
    "        \n",
    "        # Create a dictionary with query and result to pass to the chain\n",
    "        inputs = {\"query\": query, \"explanation\": explanation, \"question\" : question}\n",
    "\n",
    "        # Run the chain and catch any potential errors\n",
    "        try:\n",
    "            test_output_Claude_explanation = chain_testing_Claude_explanation.invoke(inputs)\n",
    "        except Exception as e:\n",
    "            test_output_Claude_explanation = f\"Error in row {i}: {str(e)}\"\n",
    "        \n",
    "        # Store the comparison output\n",
    "        assessment_Claude_explanation.append(test_output_Claude_explanation)\n",
    "    \n",
    "    # Add the comparison results to a new column\n",
    "    df_1b_testing_output[\"Assessment Claude Explanation\"] = assessment_Claude_explanation\n",
    "    \n",
    "    return df_1b_testing_output\n",
    "\n",
    "# Call the function and process the dataframe\n",
    "df_explanation_assessment_Claude = Explanation_testing_Claude(df_1b_testing_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Translation Assessment*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain_testing_Claude_translation = (\n",
    "    {\"query\": RunnablePassthrough(), \"translation\": RunnablePassthrough(), \"question\": RunnablePassthrough()}\n",
    "    | prompt_testing_translation\n",
    "    | Claude_model\n",
    "    | parser\n",
    ")\n",
    "\n",
    "# Function to compare each question and result using the chain\n",
    "def Translation_testing_Claude(df_1b_testing_output):\n",
    "    assessment_Claude_translation = []\n",
    "    \n",
    "    for i, row in df_1b_testing_output.iterrows():\n",
    "        # Get the question and result from the dataframe\n",
    "        query = row[\"Query\"]\n",
    "        question = row[\"Question\"]\n",
    "        translation = row[\"Translation\"]\n",
    "        \n",
    "        # Create a dictionary with query and result to pass to the chain\n",
    "        inputs = {\"query\": query, \"translation\": translation, \"question\" : question}\n",
    "\n",
    "        # Run the chain and catch any potential errors\n",
    "        try:\n",
    "            test_output_Claude_translation = chain_testing_Claude_translation.invoke(inputs)\n",
    "        except Exception as e:\n",
    "            test_output_Claude_translation = f\"Error in row {i}: {str(e)}\"\n",
    "        \n",
    "        # Store the comparison output\n",
    "        assessment_Claude_translation.append(test_output_Claude_translation)\n",
    "    \n",
    "    # Add the comparison results to a new column\n",
    "    df_1b_testing_output[\"Assessment Claude Translation\"] = assessment_Claude_translation\n",
    "    \n",
    "    return df_1b_testing_output\n",
    "\n",
    "# Call the function and process the dataframe\n",
    "df_translation_assessment_Claude = Translation_testing_Claude(df_1b_testing_output)\n",
    "\n",
    "# Save the dataframe, including the comparison, to a CSV file\n",
    "df_translation_assessment_Claude.to_csv(\"../8_Testing_Input_and_Output/LLM_assessment_1b.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mtgitenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
