{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Testing of 1a CodeGemma with RAG and Schema***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Loading packages, libraries and secrets into notebook**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the required libraries\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_openai import OpenAI\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnableParallel, RunnablePassthrough\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_anthropic import ChatAnthropic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accessing the secrets from the environment variables\n",
    "load_dotenv()\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "GOOGLE_API_KEY = os.getenv(\"GOOGLE_API_KEY\")\n",
    "ANTHROPIC_API_KEY = os.getenv(\"ANTHROPIC_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Loading data into dataframe for testing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload the dataset and transform to dataframe\n",
    "# Define the dataset path\n",
    "dataset_path = \"../8_Testing_Input_and_Output/App_Output_1a_CG.csv\"\n",
    "print(\"Dataset Path:\", dataset_path)\n",
    "\n",
    "# Check if the file exists at the specified path\n",
    "if not os.path.isfile(dataset_path):\n",
    "    raise FileNotFoundError(f\"Unable to find the file at {dataset_path}\")\n",
    "\n",
    "# Load the dataset\n",
    "testing_output_1a = load_dataset('csv', data_files=dataset_path)\n",
    "\n",
    "# Convert the dataset to a pandas dataframe\n",
    "df_1a_testing_output = testing_output_1a['train'].to_pandas()\n",
    "\n",
    "# Print a few rows to verify\n",
    "print(df_1a_testing_output.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Testing Template**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Explanation Assessment Template*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chain setup explanation\n",
    "testing_template_explanation = \"\"\"\n",
    "\"How well does the following Explanation explain the SQL Query? Please assess it critically then assign and output one of the following scores where 4 is the highest and 1 is the lowest: Acceptable (4), Minor errors (3), Major errors (2), or Unacceptable (1). To determine the score, go through the assessment step by step and consider the accuracy and understandability of the explanation assigning one score for accuracy and understandability, and a combined overall score for the explanation.\"\n",
    "\n",
    "SQL Query: {query}\n",
    "\n",
    "Explanation: {explanation}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "\n",
    "prompt_testing_explanation = ChatPromptTemplate.from_template(testing_template_explanation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Translation Assessment Template*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chain setup translation\n",
    "testing_template_translation = \"\"\"\n",
    "\"How well does the following Translation translate the SQL Query? Please assess it critically then assign and output one of the following scores where 4 is the highest and 1 is the lowest: Acceptable (4), Minor errors (3), Major errors (2), or Unacceptable (1). To determine the score, go through the assessment step by step and consider the accuracy and understandability of the translation assigning one score for accuracy and understandability, and a combined overall score for the translation.\"\n",
    "\n",
    "SQL Query: {query}\n",
    "\n",
    "Translation: {translation}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "\n",
    "prompt_testing_translation = ChatPromptTemplate.from_template(testing_template_translation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**OpenAI Assessment**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Explanation Assessment*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model and parsing setup\n",
    "model = ChatOpenAI(api_key=OPENAI_API_KEY, model=\"gpt-4o-mini\")\n",
    "parser = StrOutputParser()\n",
    "\n",
    "chain_testing_OAI_explanation = (\n",
    "    {\"query\": RunnablePassthrough(), \"explanation\": RunnablePassthrough(), \"question\": RunnablePassthrough()}\n",
    "    | prompt_testing_explanation\n",
    "    | model\n",
    "    | parser\n",
    ")\n",
    "\n",
    "# Function to compare each question and result using the chain\n",
    "def Explanation_testing_OAI(df_1a_testing_output):\n",
    "    assessment_OAI_explanation = []\n",
    "    \n",
    "    for i, row in df_1a_testing_output.iterrows():\n",
    "        # Get the question and result from the dataframe\n",
    "        query = row[\"Query\"]\n",
    "        question = row[\"Question\"]\n",
    "        explanation = row[\"Explanation\"]\n",
    "        \n",
    "        # Create a dictionary with query and result to pass to the chain\n",
    "        inputs = {\"query\": query, \"explanation\": explanation, \"question\" : question}\n",
    "\n",
    "        # Run the chain and catch any potential errors\n",
    "        try:\n",
    "            test_output_OAI_explanation = chain_testing_OAI_explanation.invoke(inputs)\n",
    "        except Exception as e:\n",
    "            test_output_OAI_explanation = f\"Error in row {i}: {str(e)}\"\n",
    "        \n",
    "        # Store the comparison output\n",
    "        assessment_OAI_explanation.append( test_output_OAI_explanation)\n",
    "    \n",
    "    # Add the comparison results to a new column\n",
    "    df_1a_testing_output[\"Assessment OAI Explanation\"] = assessment_OAI_explanation\n",
    "    \n",
    "    return df_1a_testing_output\n",
    "\n",
    "# Call the function and process the dataframe\n",
    "df_explanation_assessment_OAI_ = Explanation_testing_OAI(df_1a_testing_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Translation Assessment*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain_testing_OAI_translation = (\n",
    "    {\"query\": RunnablePassthrough(), \"translation\": RunnablePassthrough(), \"question\": RunnablePassthrough()}\n",
    "    | prompt_testing_translation\n",
    "    | model\n",
    "    | parser\n",
    ")\n",
    "\n",
    "# Function to compare each question and result using the chain\n",
    "def Translation_testing_OAI(df_1a_testing_output):\n",
    "    assessment_OAI_translation = []\n",
    "    \n",
    "    for i, row in df_1a_testing_output.iterrows():\n",
    "        # Get the question and result from the dataframe\n",
    "        query = row[\"Query\"]\n",
    "        translation = row[\"Translation\"]\n",
    "        question = row[\"Question\"]\n",
    "        \n",
    "        # Create a dictionary with query and result to pass to the chain\n",
    "        inputs = {\"query\": query, \"translation\": translation, \"question\" : question}\n",
    "\n",
    "        # Run the chain and catch any potential errors\n",
    "        try:\n",
    "            test_output_OAI_translation = chain_testing_OAI_translation.invoke(inputs)\n",
    "        except Exception as e:\n",
    "            test_output_OAI_translation = f\"Error in row {i}: {str(e)}\"\n",
    "        \n",
    "        # Store the comparison output\n",
    "        assessment_OAI_translation.append( test_output_OAI_translation)\n",
    "    \n",
    "    # Add the comparison results to a new column\n",
    "    df_1a_testing_output[\"Assessment OAI Translation\"] = assessment_OAI_translation\n",
    "    \n",
    "    return df_1a_testing_output\n",
    "\n",
    "# Call the function and process the dataframe\n",
    "df_translation_assessment_OAI = Translation_testing_OAI(df_1a_testing_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Gemini Assessment**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Explanation Assessment*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "Gemini_model = ChatGoogleGenerativeAI(model=\"gemini-pro\", api_key=GOOGLE_API_KEY)\n",
    "\n",
    "\n",
    "chain_testing_Gemi_explanation = (\n",
    "    {\"query\": RunnablePassthrough(), \"explanation\": RunnablePassthrough(), \"question\": RunnablePassthrough()}\n",
    "    | prompt_testing_explanation\n",
    "    | Gemini_model\n",
    "    | parser\n",
    ")\n",
    "\n",
    "# Function to compare each question and result using the chain\n",
    "def Explanation_testing_Gemi(df_1a_testing_output):\n",
    "    assessment_Gemi_explanation = []\n",
    "    \n",
    "    for i, row in df_1a_testing_output.iterrows():\n",
    "        # Get the question and result from the dataframe\n",
    "        query = row[\"Query\"]\n",
    "        question = row[\"Question\"]\n",
    "        explanation = row[\"Explanation\"]\n",
    "        \n",
    "        # Create a dictionary with query and result to pass to the chain\n",
    "        inputs = {\"query\": query, \"explanation\": explanation, \"question\" : question}\n",
    "\n",
    "        # Run the chain and catch any potential errors\n",
    "        try:\n",
    "            test_output_Gemi_explanation = chain_testing_Gemi_explanation.invoke(inputs)\n",
    "        except Exception as e:\n",
    "            test_output_Gemi_explanation = f\"Error in row {i}: {str(e)}\"\n",
    "        \n",
    "        # Store the comparison output\n",
    "        assessment_Gemi_explanation.append(test_output_Gemi_explanation)\n",
    "    \n",
    "    # Add the comparison results to a new column\n",
    "    df_1a_testing_output[\"Assessment Gemini Explanation\"] = assessment_Gemi_explanation\n",
    "    \n",
    "    return df_1a_testing_output\n",
    "\n",
    "# Call the function and process the dataframe\n",
    "df_explanation_assessment_Gemi = Explanation_testing_Gemi(df_1a_testing_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Translation Assessment*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain_testing_Gemi_translation = (\n",
    "    {\"query\": RunnablePassthrough(), \"translation\": RunnablePassthrough(), \"question\": RunnablePassthrough()}\n",
    "    | prompt_testing_translation\n",
    "    | Gemini_model\n",
    "    | parser\n",
    ")\n",
    "\n",
    "# Function to compare each question and result using the chain\n",
    "def Translation_testing_Gemi(df_1a_testing_output):\n",
    "    assessment_Gemi_translation = []\n",
    "    \n",
    "    for i, row in df_1a_testing_output.iterrows():\n",
    "        # Get the question and result from the dataframe\n",
    "        query = row[\"Query\"]\n",
    "        question = row[\"Question\"]\n",
    "        translation = row[\"Translation\"]\n",
    "        \n",
    "        # Create a dictionary with query and result to pass to the chain\n",
    "        inputs = {\"query\": query, \"translation\": translation, \"question\" : question}\n",
    "\n",
    "        # Run the chain and catch any potential errors\n",
    "        try:\n",
    "            test_output_Gemi_translation = chain_testing_Gemi_translation.invoke(inputs)\n",
    "        except Exception as e:\n",
    "            test_output_Gemi_translation = f\"Error in row {i}: {str(e)}\"\n",
    "        \n",
    "        # Store the comparison output\n",
    "        assessment_Gemi_translation.append(test_output_Gemi_translation)\n",
    "    \n",
    "    # Add the comparison results to a new column\n",
    "    df_1a_testing_output[\"Assessment Gemini Translation\"] = assessment_Gemi_translation\n",
    "    \n",
    "    return df_1a_testing_output\n",
    "\n",
    "# Call the function and process the dataframe\n",
    "df_translation_assessment_Gemi = Translation_testing_Gemi(df_1a_testing_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Claude Assessment**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Explanation Assessment*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "Claude_model = ChatAnthropic(model=\"claude-3-5-sonnet-20240620\", api_key=ANTHROPIC_API_KEY)\n",
    "\n",
    "\n",
    "chain_testing_Claude_explanation = (\n",
    "    {\"query\": RunnablePassthrough(), \"explanation\": RunnablePassthrough(), \"question\": RunnablePassthrough()}\n",
    "    | prompt_testing_explanation\n",
    "    | Claude_model\n",
    "    | parser\n",
    ")\n",
    "\n",
    "# Function to compare each question and result using the chain\n",
    "def Explanation_testing_Claude(df_1a_testing_output):\n",
    "    assessment_Claude_explanation = []\n",
    "    \n",
    "    for i, row in df_1a_testing_output.iterrows():\n",
    "        # Get the question and result from the dataframe\n",
    "        query = row[\"Query\"]\n",
    "        question = row[\"Question\"]\n",
    "        explanation = row[\"Explanation\"]\n",
    "        \n",
    "        # Create a dictionary with query and result to pass to the chain\n",
    "        inputs = {\"query\": query, \"explanation\": explanation, \"question\" : question}\n",
    "\n",
    "        # Run the chain and catch any potential errors\n",
    "        try:\n",
    "            test_output_Claude_explanation = chain_testing_Claude_explanation.invoke(inputs)\n",
    "        except Exception as e:\n",
    "            test_output_Claude_explanation = f\"Error in row {i}: {str(e)}\"\n",
    "        \n",
    "        # Store the comparison output\n",
    "        assessment_Claude_explanation.append(test_output_Claude_explanation)\n",
    "    \n",
    "    # Add the comparison results to a new column\n",
    "    df_1a_testing_output[\"Assessment Claude Explanation\"] = assessment_Claude_explanation\n",
    "    \n",
    "    return df_1a_testing_output\n",
    "\n",
    "# Call the function and process the dataframe\n",
    "df_explanation_assessment_Claude = Explanation_testing_Claude(df_1a_testing_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Translation Assessment*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain_testing_Claude_translation = (\n",
    "    {\"query\": RunnablePassthrough(), \"translation\": RunnablePassthrough(), \"question\": RunnablePassthrough()}\n",
    "    | prompt_testing_translation\n",
    "    | Claude_model\n",
    "    | parser\n",
    ")\n",
    "\n",
    "# Function to compare each question and result using the chain\n",
    "def Translation_testing_Claude(df_1a_testing_output):\n",
    "    assessment_Claude_translation = []\n",
    "    \n",
    "    for i, row in df_1a_testing_output.iterrows():\n",
    "        # Get the question and result from the dataframe\n",
    "        query = row[\"Query\"]\n",
    "        question = row[\"Question\"]\n",
    "        translation = row[\"Translation\"]\n",
    "        \n",
    "        # Create a dictionary with query and result to pass to the chain\n",
    "        inputs = {\"query\": query, \"translation\": translation, \"question\" : question}\n",
    "\n",
    "        # Run the chain and catch any potential errors\n",
    "        try:\n",
    "            test_output_Claude_translation = chain_testing_Claude_translation.invoke(inputs)\n",
    "        except Exception as e:\n",
    "            test_output_Claude_translation = f\"Error in row {i}: {str(e)}\"\n",
    "        \n",
    "        # Store the comparison output\n",
    "        assessment_Claude_translation.append(test_output_Claude_translation)\n",
    "    \n",
    "    # Add the comparison results to a new column\n",
    "    df_1a_testing_output[\"Assessment Claude Translation\"] = assessment_Claude_translation\n",
    "    \n",
    "    return df_1a_testing_output\n",
    "\n",
    "# Call the function and process the dataframe\n",
    "df_translation_assessment_Claude = Translation_testing_Claude(df_1a_testing_output)\n",
    "\n",
    "# Save the dataframe, including the comparison, to a CSV file\n",
    "df_translation_assessment_Claude.to_csv(\"../8_Testing_Input_and_Output/LLM_assessment_1a_CG.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mtgitenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
