{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Gemma RAG LLM setup**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook should be run in Google Colab or similar site, where high GPU processing power is available. In Google Colab, the A100 GPU works best."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Loading packages, libraries and secrets into notebook**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In Google Colab Mounting Google Drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installing the required packages\n",
    "!pip install pandas==2.1.4 numpy==1.23.5 pymongo gradio langchain_mongodb sentence_transformers tensorflow==2.15\n",
    "!pip install -U transformers\n",
    "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
    "# install below if using GPU\n",
    "!pip install accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the required functions and modules\n",
    "import gradio as gr\n",
    "from gradio.themes.base import Base\n",
    "from sentence_transformers import SentenceTransformer # https://huggingface.co/thenlper/gte-large\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from transformers import AutoConfig\n",
    "import torch\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the required functions and modules\n",
    "import gradio as gr\n",
    "from gradio.themes.base import Base\n",
    "from sentence_transformers import SentenceTransformer # https://huggingface.co/thenlper/gte-large\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from transformers import AutoConfig\n",
    "import torch\n",
    "import gc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Accessing secrets***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accessing the secrets from the environment variables\n",
    "#load_dotenv()\n",
    "#MONGO_URI_SQL = os.getenv(\"MONGO_URI_SQL\")\n",
    "#MONGO_URI_schema = os.getenv(\"MONGO_URI_Schema\")\n",
    "#HF_Token = os.getenv(\"HF_TOKEN\")\n",
    "\n",
    "# In Google Colab, you can use the following code to access the secret\n",
    "from google.colab import userdata\n",
    "HF_Token = userdata.get('HF_TOKEN')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Loading the Tokenizer and LLM-Model***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The 7 billion Gemma model version has been selected for better performance, however a 2 billion version exists, requiring less processing power. To use the 2 billion version, the \"7b\" in the code below can be swapped for \"2b\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-7b-it\")\n",
    "# CPU Enabled uncomment below 👇🏽\n",
    "# model = AutoModelForCausalLM.from_pretrained(\"google/gemma-7b-it\")\n",
    "# GPU Enabled use below 👇🏽\n",
    "model = AutoModelForCausalLM.from_pretrained(\"google/gemma-7b-it\", device_map=\"auto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"SELECT professional_id ,  last_name ,  cell_number FROM Professionals WHERE state  =  'Indiana' UNION SELECT T1.professional_id ,  T1.last_name ,  T1.cell_number FROM Professionals AS T1 JOIN Treatments AS T2 ON T1.professional_id  =  T2.professional_id GROUP BY T1.professional_id HAVING count(*)  >  2\"\n",
    "\n",
    "output_length = len(query.split())*3 # word count of SQL query multiplied by four\n",
    "\n",
    "def process_query(query):\n",
    "    # Generate response\n",
    "    def generate_response(query):\n",
    "        combined_information = (\n",
    "            f\"Instructions: Generate a natural language Translation stating what the Query wants to achieve followed by an Explanation stating how the Query is composed and how it works.\"\n",
    "            f\"Go through it step by step and formulate the Translation and Explanation in simple and concise language.\"\n",
    "            f\"Keep the word count in line with the Length number.\\n\\n\"\n",
    "            f\"Length: {output_length}\"\n",
    "            f\"Query: {query}\\n\\n\"\n",
    "            f\"Response:\\n\"\n",
    "        )\n",
    "\n",
    "        # Moving tensors to GPU and generating a response\n",
    "        input_ids = tokenizer(combined_information, return_tensors=\"pt\").to(\"cuda\")\n",
    "        response = model.generate(**input_ids, max_new_tokens=1000)\n",
    "        decoded_response = tokenizer.decode(response[0], skip_special_tokens=True).strip()\n",
    "\n",
    "        # Post-processing: Extracting the content after 'Response:\\n'\n",
    "        if \"Response:\" in decoded_response:\n",
    "            decoded_response = decoded_response.split(\"Response:\", 1)[-1].strip()\n",
    "\n",
    "        # Clear GPU memory for `input_ids` and `response`\n",
    "        del input_ids, response\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "\n",
    "        return decoded_response\n",
    "\n",
    "    # Return the final generated response\n",
    "    return generate_response(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Chat interface setup***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Change cell type below to Python, when running only this script. Markdown format for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a web interface for the app, using Gradio\n",
    "with gr.Blocks(theme=Base(), title=\"Question Answering App using Vector Search + RAG\") as demo:\n",
    "    gr.Markdown(\n",
    "        \"\"\"\n",
    "        # Question Answering App using Atlas Vector Search + RAG Architecture\n",
    "        \"\"\")\n",
    "    textbox = gr.Textbox(label=\"Enter your SQL statement:\")\n",
    "    with gr.Row():\n",
    "        button = gr.Button(\"Submit\", variant=\"primary\")\n",
    "    with gr.Column():\n",
    "        output = gr.Textbox(lines=1, max_lines=30, label=\"Natural language translation and explanation:\")\n",
    "\n",
    "# Call chain_invoke function upon clicking the Submit button\n",
    "\n",
    "    button.click(process_query, textbox, outputs=output)\n",
    "\n",
    "demo.launch()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
