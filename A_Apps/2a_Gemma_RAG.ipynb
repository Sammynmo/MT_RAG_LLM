{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Gemma RAG LLM setup**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook should be run in Google Colab or similar site, where high GPU processing power is available. In Google Colab, the A100 GPU works best."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Loading packages, libraries and secrets into notebook**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In Google Colab Mounting Google Drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installing the required packages\n",
    "!pip install pandas==2.1.4 numpy==1.23.5 pymongo gradio langchain_mongodb sentence_transformers tensorflow==2.15\n",
    "!pip install -U transformers\n",
    "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
    "# install below if using GPU\n",
    "!pip install accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the required functions and modules\n",
    "from pymongo import MongoClient\n",
    "from langchain_mongodb import MongoDBAtlasVectorSearch\n",
    "import gradio as gr\n",
    "from gradio.themes.base import Base\n",
    "from sentence_transformers import SentenceTransformer # https://huggingface.co/thenlper/gte-large\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from transformers import AutoConfig\n",
    "import torch\n",
    "import gc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Accessing secrets***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accessing the secrets from the environment variables\n",
    "#load_dotenv()\n",
    "#MONGO_URI_SQL = os.getenv(\"MONGO_URI_SQL\")\n",
    "#MONGO_URI_schema = os.getenv(\"MONGO_URI_Schema\")\n",
    "#HF_Token = os.getenv(\"HF_TOKEN\")\n",
    "\n",
    "# In Google Colab, you can use the following code to access the secret\n",
    "from google.colab import userdata\n",
    "MONGO_URI_SQL = userdata.get('MONGO_URI_SQL')\n",
    "MONGO_URI_schema = userdata.get('MONGO_URI_Schema')\n",
    "HF_Token = userdata.get('HF_TOKEN')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Generating the embedding***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embedding model setup\n",
    "embedding_model = SentenceTransformer(\"thenlper/gte-large\")\n",
    "\n",
    "class CustomEmbeddingFunction:\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "\n",
    "    def embed_documents(self, texts):\n",
    "        \"\"\"Embeds a list of documents.\"\"\"\n",
    "        embeddings = self.model.encode(texts)\n",
    "        return embeddings.tolist()\n",
    "\n",
    "    def embed_query(self, text):\n",
    "        \"\"\"Embeds a single query.\"\"\"\n",
    "        embedding = self.model.encode(text)\n",
    "        return embedding.tolist()\n",
    "\n",
    "# Wrap the SentenceTransformer model\n",
    "embedding_function = CustomEmbeddingFunction(embedding_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## MongoDB setup\n",
    "# SQL Vector\n",
    "client_SQL = MongoClient(MONGO_URI_SQL)\n",
    "dbName_SQL = \"MVector\"\n",
    "collectionName_SQL = \"MTSQL\"\n",
    "collection_SQL = client_SQL[dbName_SQL][collectionName_SQL]\n",
    "index_name_SQL = \"vector_index_SQL\"\n",
    "\n",
    "## SQL Vector setup\n",
    "# Vector store setup\n",
    "vector_store_SQL = MongoDBAtlasVectorSearch(\n",
    "    client=client_SQL,\n",
    "    database=dbName_SQL,\n",
    "    collection=collection_SQL,\n",
    "    index_name=index_name_SQL,\n",
    "    embedding=embedding_function,\n",
    "    text_key=\"Query\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Loading the Tokenizer and LLM-Model***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The 7 billion Gemma model version has been selected for better performance, however a 2 billion version exists, requiring less processing power. To use the 2 billion version, the \"7b\" in the code below can be swapped for \"2b\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-7b-it\")\n",
    "# CPU Enabled uncomment below üëáüèΩ\n",
    "# model = AutoModelForCausalLM.from_pretrained(\"google/gemma-7b-it\")\n",
    "# GPU Enabled use below üëáüèΩ\n",
    "model = AutoModelForCausalLM.from_pretrained(\"google/gemma-7b-it\", device_map=\"auto\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Chain setup***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\n",
    "output_length = len(query.split())*3 # word count of SQL query multiplied by four\n",
    "\n",
    "def process_query_RAG(query):\n",
    "    # SQL Vector setup\n",
    "    retriever_SQL = vector_store_SQL.as_retriever(search_kwargs={\"k\": 4})\n",
    "\n",
    "    # Retrieve SQL documents\n",
    "    def logging_retriever_function_SQL(retriever_SQL, query):\n",
    "        documents_SQL = retriever_SQL.invoke(query)\n",
    "        print(\"Retrieved Documents:\")\n",
    "        for doc in documents_SQL:\n",
    "            print(doc)\n",
    "        return documents_SQL\n",
    "\n",
    "    def get_source_information_SQL(query):\n",
    "        retrieved_docs = logging_retriever_function_SQL(retriever_SQL, query)\n",
    "        source_information_SQL = \"\\n\".join([str(doc) for doc in retrieved_docs])\n",
    "        return source_information_SQL\n",
    "\n",
    "    # Retrieve SQL information\n",
    "    information_summary_SQL = get_source_information_SQL(query)\n",
    "\n",
    "    # Generate response\n",
    "    def generate_response(query):\n",
    "        combined_information = ( \n",
    "            f\"Instructions: Generate a natural language Translation stating what the Query wants to achieve followed by an Explanation stating how the Query is composed and how it works.\"\n",
    "            f\"Go through it step by step and formulate the Translation and Explanation in simple and concise language.\"\n",
    "            f\"Use the information of the Context as examples for the translation.\"\n",
    "            f\"Keep the word count in line with the Length number.\\n\\n\"\n",
    "            f\"Query: {query}\\n\\n\"\n",
    "            f\"Context: {information_summary_SQL}\\n\\n\"\n",
    "            f\"Length: {output_length}\\n\\n\"            \n",
    "            f\"Response:\\n\"\n",
    "        )\n",
    "\n",
    "        # Moving tensors to GPU and generating a response\n",
    "        input_ids = tokenizer(combined_information, return_tensors=\"pt\").to(\"cuda\")\n",
    "        response = model.generate(**input_ids, max_new_tokens=1000)\n",
    "        decoded_response = tokenizer.decode(response[0], skip_special_tokens=True).strip()\n",
    "\n",
    "        # Post-processing: Extracting the content after 'Response:\\n'\n",
    "        if \"Response:\" in decoded_response:\n",
    "            decoded_response = decoded_response.split(\"Response:\", 1)[-1].strip()\n",
    "\n",
    "        # Clear GPU memory for `input_ids` and `response`\n",
    "        del input_ids, response\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "\n",
    "        return decoded_response\n",
    "\n",
    "    # Return the final generated response\n",
    "    return generate_response(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Chat interface setup***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Change cell type below to Python, when running only this script. Markdown format for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a web interface for the app, using Gradio\n",
    "with gr.Blocks(theme=Base(), title=\"Question Answering App using Vector Search + RAG\") as demo:\n",
    "    gr.Markdown(\n",
    "        \"\"\"\n",
    "        # Question Answering App using Atlas Vector Search + RAG Architecture\n",
    "        \"\"\")\n",
    "    textbox = gr.Textbox(label=\"Enter your SQL statement:\")\n",
    "    with gr.Row():\n",
    "        button = gr.Button(\"Submit\", variant=\"primary\")\n",
    "    with gr.Column():\n",
    "        output = gr.Textbox(lines=1, max_lines=30, label=\"Natural language translation and explanation:\")\n",
    "\n",
    "# Call chain_invoke function upon clicking the Submit button\n",
    "\n",
    "    button.click(process_query_RAG, textbox, outputs=output)\n",
    "\n",
    "demo.launch()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
