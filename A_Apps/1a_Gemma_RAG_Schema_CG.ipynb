{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4-MF5Y0p_Ecc"
   },
   "source": [
    "***Code Gemma RAG LLM setup with Schema***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Gw7o5VW__Ecd"
   },
   "source": [
    "This notebook should be run in Google Colab or similar site, where high GPU processing power is available. In Google Colab, the A100 GPU works best."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Loading packages, libraries and secrets into notebook**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GAKd6mz5ZwKi"
   },
   "outputs": [],
   "source": [
    "# Mounting Google Drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DLDK51av_rgy"
   },
   "outputs": [],
   "source": [
    "# Installing the required packages\n",
    "!pip install pandas==2.1.4 numpy==1.23.5 pymongo gradio langchain_mongodb sentence_transformers tensorflow==2.15\n",
    "!pip install -U transformers\n",
    "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
    "# install below if using GPU\n",
    "!pip install accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Bj1R3wyD-obf"
   },
   "outputs": [],
   "source": [
    "# Importing the required functions and modules\n",
    "from pymongo import MongoClient\n",
    "from langchain_mongodb import MongoDBAtlasVectorSearch\n",
    "import gradio as gr\n",
    "from gradio.themes.base import Base\n",
    "from sentence_transformers import SentenceTransformer # https://huggingface.co/thenlper/gte-large\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from transformers import AutoConfig\n",
    "import torch\n",
    "import gc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Accessing secrets***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Sm-asslY-w79"
   },
   "outputs": [],
   "source": [
    "# Accessing the secrets from the environment variables\n",
    "#load_dotenv()\n",
    "#MONGO_URI_SQL = os.getenv(\"MONGO_URI_SQL\")\n",
    "#MONGO_URI_schema = os.getenv(\"MONGO_URI_Schema\")\n",
    "#HF_Token = os.getenv(\"HF_TOKEN\")\n",
    "\n",
    "# In Google Colab, you can use the following code to access the secret\n",
    "from google.colab import userdata\n",
    "MONGO_URI_SQL = userdata.get('MONGO_URI_SQL')\n",
    "MONGO_URI_schema = userdata.get('MONGO_URI_Schema')\n",
    "HF_Token = userdata.get('HF_TOKEN')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0MkVbLTA_Ecg"
   },
   "source": [
    "***Generating the embedding***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VGo9YcJu_Ech"
   },
   "outputs": [],
   "source": [
    "# Embedding model setup\n",
    "embedding_model = SentenceTransformer(\"thenlper/gte-large\")\n",
    "\n",
    "class CustomEmbeddingFunction:\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "\n",
    "    def embed_documents(self, texts):\n",
    "        \"\"\"Embeds a list of documents.\"\"\"\n",
    "        embeddings = self.model.encode(texts)\n",
    "        return embeddings.tolist()\n",
    "\n",
    "    def embed_query(self, text):\n",
    "        \"\"\"Embeds a single query.\"\"\"\n",
    "        embedding = self.model.encode(text)\n",
    "        return embedding.tolist()\n",
    "\n",
    "# Wrap the SentenceTransformer model\n",
    "embedding_function = CustomEmbeddingFunction(embedding_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tFi9zNY0KDYS"
   },
   "source": [
    "***Vector DB Setup***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CNSKRZA5_Ech"
   },
   "outputs": [],
   "source": [
    "## MongoDB setup\n",
    "# SQL Vector\n",
    "client_SQL = MongoClient(MONGO_URI_SQL)\n",
    "dbName_SQL = \"MVector\"\n",
    "collectionName_SQL = \"MTSQL\"\n",
    "collection_SQL = client_SQL[dbName_SQL][collectionName_SQL]\n",
    "index_name_SQL = \"vector_index_SQL\"\n",
    "\n",
    "## SQL Vector setup\n",
    "# Vector store setup\n",
    "vector_store_SQL = MongoDBAtlasVectorSearch(\n",
    "    client=client_SQL,\n",
    "    database=dbName_SQL,\n",
    "    collection=collection_SQL,\n",
    "    index_name=index_name_SQL,\n",
    "    embedding=embedding_function,\n",
    "    text_key=\"Query\"\n",
    ")\n",
    "\n",
    "# Schema Vector\n",
    "client_schema = MongoClient(MONGO_URI_schema)\n",
    "dbName_schema = \"MVector\"\n",
    "collectionName_schema = \"MTSchemaAll\"\n",
    "collection_schema = client_schema[dbName_schema][collectionName_schema]\n",
    "index_name_schema = \"vector_index_schema_all\"\n",
    "\n",
    "## Schema Vector setup\n",
    "# Vector store setup\n",
    "vector_store_schema = MongoDBAtlasVectorSearch(\n",
    "    client=client_schema,\n",
    "    database=dbName_schema,\n",
    "    collection=collection_schema,\n",
    "    index_name=index_name_schema,\n",
    "    embedding=embedding_function,\n",
    "    text_key=\"Lookup_name\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1Mo2HmZr_Ech"
   },
   "source": [
    "***Loading the Tokenizer and LLM-Model***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The 7 billion Gemma model version has been selected for better performance, however a 2 billion version exists, requiring less processing power. To use the 2 billion version, the \"7b\" in the code below can be swapped for \"2b\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AzCvel0O_Ech"
   },
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"google/codegemma-7b-it\")\n",
    "# CPU Enabled uncomment below üëáüèΩ\n",
    "# model = AutoModelForCausalLM.from_pretrained(\"google/codegemma-7b-it\")\n",
    "# GPU Enabled use below üëáüèΩ\n",
    "model = AutoModelForCausalLM.from_pretrained(\"google/codegemma-7b-it\", device_map=\"auto\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SG1Oq2Mv_Eci"
   },
   "source": [
    "***Chain setup***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jSupM3Jf_Eci"
   },
   "outputs": [],
   "source": [
    "query=\"\"\n",
    "\n",
    "output_length = len(query.split())*3 # word count of SQL query multiplied by three\n",
    "\n",
    "DB_name = \"\"\n",
    "\n",
    "input_value = DB_name + query\n",
    "\n",
    "\n",
    "def process_query_schema(query, input_value):\n",
    "    # SQL Vector setup\n",
    "    retriever_SQL = vector_store_SQL.as_retriever(search_kwargs={\"k\": 4})\n",
    "\n",
    "    # Retrieve SQL documents\n",
    "    def logging_retriever_function_SQL(retriever_SQL, query):\n",
    "        documents_SQL = retriever_SQL.invoke(query)\n",
    "        print(\"Retrieved Documents:\")\n",
    "        for doc in documents_SQL:\n",
    "            print(doc)\n",
    "        return documents_SQL\n",
    "\n",
    "    def get_source_information_SQL(query):\n",
    "        retrieved_docs = logging_retriever_function_SQL(retriever_SQL, query)\n",
    "        source_information_SQL = \"\\n\".join([str(doc) for doc in retrieved_docs])\n",
    "        return source_information_SQL\n",
    "\n",
    "    # Retrieve SQL information\n",
    "    information_summary_SQL = get_source_information_SQL(query)\n",
    "\n",
    "    # Schema Vector setup\n",
    "    retriever_schema = vector_store_schema.as_retriever(search_kwargs={\"k\": 10})\n",
    "\n",
    "    # Retrieve schema documents\n",
    "    def logging_retriever_function_schema(retriever_schema, input_value):\n",
    "        documents_schema = retriever_schema.invoke(input_value)\n",
    "        print(\"Retrieved Schema:\")\n",
    "        for doc in documents_schema:\n",
    "            print(doc)\n",
    "        return documents_schema\n",
    "\n",
    "    def get_source_information_schema(input_value):\n",
    "        retrieved_docs = logging_retriever_function_schema(retriever_schema, input_value)\n",
    "        source_information_schema = \"\\n\".join([str(doc) for doc in retrieved_docs])\n",
    "        return source_information_schema\n",
    "\n",
    "    # Retrieve schema information\n",
    "    information_summary_schema = get_source_information_schema(input_value)\n",
    "\n",
    "    # Generate response\n",
    "    def generate_response(query):\n",
    "        combined_information = (\n",
    "            f\"Instructions: Generate a natural language Translation stating what the Query wants to achieve followed by an Explanation stating how the Query is composed and how it works.\"\n",
    "            f\"Go through it step by step and formulate the Translation and Explanation in simple and concise language.\"\n",
    "            f\"Use the information of the Context as examples for the translation.\"\n",
    "            f\"Keep the word count in line with the Length number.\\n\\n\"\n",
    "            f\"Query: {query}\\n\\n\"\n",
    "            f\"Input-value: {input_value}\\n\\n\"\n",
    "            f\"Context: {information_summary_SQL}\\n\\n\"\n",
    "            f\"Schema: {information_summary_schema}\\n\\n\"\n",
    "            f\"Length: {output_length}\\n\\n\"\n",
    "            f\"Response:\\n\"\n",
    "        )\n",
    "\n",
    "        # Moving tensors to GPU and generating a response\n",
    "        input_ids = tokenizer(combined_information, return_tensors=\"pt\").to(\"cuda\")\n",
    "        response = model.generate(**input_ids, max_new_tokens=1000)\n",
    "        decoded_response = tokenizer.decode(response[0], skip_special_tokens=True).strip()\n",
    "\n",
    "        # Post-processing: Extracting the content after 'Response:\\n'\n",
    "        if \"Response:\" in decoded_response:\n",
    "            decoded_response = decoded_response.split(\"Response:\", 1)[-1].strip()\n",
    "\n",
    "        # Clear GPU memory for `input_ids` and `response`\n",
    "        del input_ids, response\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "\n",
    "        return decoded_response\n",
    "\n",
    "    # Return the final generated response\n",
    "    return generate_response(query)\n",
    "\n",
    "def chain_1a_invoke(query, DB_name):\n",
    "    input_value = query if not DB_name else DB_name + query\n",
    "    # Execute the chain with the logging retriever\n",
    "    result = process_query_schema(query, input_value)\n",
    "    # Return the result\n",
    "    return result\n",
    "\n",
    "# Call the function and print the output\n",
    "output = chain_1a_invoke(query, DB_name)\n",
    "print(\"Generated Response:\")\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0Xv8O-5a_Eci"
   },
   "source": [
    "***Chat interface setup***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RI-xm7aNKWvd"
   },
   "source": [
    "Markdown format of Chat interface setup for testing.\n",
    "\n",
    "Change cell type below to Python, when running only this script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TguYGrWbKSNG"
   },
   "outputs": [],
   "source": [
    "def chain_1a_invoke(query, DB_name):\n",
    "    input_value = query if not DB_name else DB_name + query\n",
    "    # Execute the chain with the logging retriever\n",
    "    result = process_query_schema(query, input_value)\n",
    "    # Return the result\n",
    "    return result\n",
    "\n",
    "# Create a web interface for the app, using Gradio\n",
    "with gr.Blocks(theme=Base(), title=\"Question Answering App using Vector Search + RAG\") as demo:\n",
    "    gr.Markdown(\n",
    "        \"\"\"\n",
    "        # Question Answering App using Atlas Vector Search + RAG Architecture\n",
    "        \"\"\")\n",
    "    query = gr.Textbox(label=\"Enter your SQL statement:\")\n",
    "    DB_name = gr.Textbox(label=\"Enter the database name: (Optional)\")\n",
    "    with gr.Row():\n",
    "        button = gr.Button(\"Submit\", variant=\"primary\")\n",
    "    output = gr.Textbox(lines=1, max_lines=30, label=\"Natural language translation and explanation:\")\n",
    "\n",
    "# Call chain_invoke function upon clicking the Submit button\n",
    "\n",
    "    button.click(chain_1a_invoke, inputs=[query, DB_name], outputs=output)\n",
    "\n",
    "demo.launch()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
