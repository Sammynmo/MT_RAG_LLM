{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***OPENAI RAG LLM setup***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Loading packages, libraries and secrets into notebook**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the required libraries\n",
    "from pymongo import MongoClient\n",
    "from langchain_mongodb import MongoDBAtlasVectorSearch\n",
    "from langchain_openai import OpenAI\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.chains import RetrievalQA\n",
    "import gradio as gr\n",
    "from gradio.themes.base import Base\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnableParallel, RunnablePassthrough\n",
    "from sentence_transformers import SentenceTransformer # https://huggingface.co/thenlper/gte-large\n",
    "import os\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accessing the secrets from the environment variables\n",
    "load_dotenv()\n",
    "MONGO_URI_SQL = os.getenv(\"MONGO_URI_SQL\")\n",
    "MONGO_URI_schema = os.getenv(\"MONGO_URI_Schema\")\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "HF_Token = os.getenv(\"HF_TOKEN\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Generating the embedding***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embedding model setup\n",
    "embedding_model = SentenceTransformer(\"thenlper/gte-large\")\n",
    "\n",
    "class CustomEmbeddingFunction:\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "\n",
    "    def embed_documents(self, texts):\n",
    "        \"\"\"Embeds a list of documents.\"\"\"\n",
    "        embeddings = self.model.encode(texts)\n",
    "        return embeddings.tolist()\n",
    "\n",
    "    def embed_query(self, text):\n",
    "        \"\"\"Embeds a single query.\"\"\"\n",
    "        embedding = self.model.encode(text)\n",
    "        return embedding.tolist()\n",
    "\n",
    "# Wrap the SentenceTransformer model\n",
    "embedding_function = CustomEmbeddingFunction(embedding_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## MongoDB setup\n",
    "# SQL Vector\n",
    "client_SQL = MongoClient(MONGO_URI_SQL)\n",
    "dbName_SQL = \"MVector\"\n",
    "collectionName_SQL = \"MTSQL\"\n",
    "collection_SQL = client_SQL[dbName_SQL][collectionName_SQL]\n",
    "index_name_SQL = \"vector_index_SQL\"\n",
    "\n",
    "## SQL Vector setup\n",
    "# Vector store setup\n",
    "vector_store_SQL = MongoDBAtlasVectorSearch(\n",
    "    client=client_SQL,\n",
    "    database=dbName_SQL,\n",
    "    collection=collection_SQL,\n",
    "    index_name=index_name_SQL,\n",
    "    embedding=embedding_function,\n",
    "    text_key=\"Query\"  \n",
    ")\n",
    "\n",
    "# Retriever setup\n",
    "retriever_SQL = vector_store_SQL.as_retriever(search_kwargs={\"k\": 4})\n",
    "\n",
    "# Define a custom logging retriever to see what the retriever is passing on\n",
    "class LoggingRetrieverSQL:\n",
    "    def __init__(self, retriever_SQL):\n",
    "        self.retriever_SQL = retriever_SQL\n",
    "\n",
    "    def __call__(self, query):\n",
    "        # Retrieve the documents\n",
    "        documents_SQL = self.retriever_SQL.invoke(query)\n",
    "        \n",
    "        # Log or print the retrieved documents\n",
    "        print(\"Retrieved Documents:\")\n",
    "        for doc in documents_SQL:\n",
    "            print(doc)\n",
    "        \n",
    "        # Return the retrieved documents\n",
    "        return documents_SQL\n",
    "\n",
    "# Wrap your retriever with the logging retriever\n",
    "logging_retriever_SQL = LoggingRetrieverSQL(retriever_SQL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Chain setup***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"SELECT T1.Name FROM conductor AS T1 JOIN orchestra AS T2 ON T1.Conductor_ID  =  T2.Conductor_ID GROUP BY T2.Conductor_ID ORDER BY COUNT(*) DESC LIMIT 1\"\n",
    "output_length = len(query.split())*3 # word count of SQL query multiplied by four\n",
    "\n",
    "# Model and parsing setup\n",
    "model = ChatOpenAI(api_key=OPENAI_API_KEY, model=\"gpt-4o-mini\", temperature = 0)\n",
    "parser = StrOutputParser()\n",
    "\n",
    "\n",
    "# Define prompt template\n",
    "template = \"\"\"\n",
    "Provide first a natural language Translation followed by an Explanation of the SQL Query. Go through it step by step and output the result in simple and concise language. Use the information of the Context as examples for the translation. Keep the output in line with the Length number.\n",
    "\n",
    "Context: {context}\n",
    "\n",
    "Query: {query}\n",
    "\n",
    "Lenght: {output_length}\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "# Chain setup\n",
    "chain_2b = (\n",
    "    {\"context\": logging_retriever_SQL, \"query\": RunnablePassthrough(), \"output_length\" : RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | model\n",
    "    | parser\n",
    ")\n",
    "\n",
    "# Execute the chain with the logging retriever\n",
    "chain_2b.invoke(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Chat interface setup***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Markdown format of Chat interface setup for testing.\n",
    "\n",
    "Change cell type below to Python, when running only this script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the chain_invoke function\n",
    "def chain_2b_invoke(query):\n",
    "    # Execute the chain with the logging retriever\n",
    "    result = chain_2b.invoke(query)\n",
    "    # Return the result \n",
    "    return result\n",
    "\n",
    "# Create a web interface for the app, using Gradio\n",
    "with gr.Blocks(theme=Base(), title=\"Question Answering App using Vector Search + RAG\") as demo:\n",
    "    gr.Markdown(\n",
    "        \"\"\"\n",
    "        # Question Answering App using Atlas Vector Search + RAG Architecture\n",
    "        \"\"\")\n",
    "    textbox = gr.Textbox(label=\"Enter your SQL statement:\")\n",
    "    with gr.Row():\n",
    "        button = gr.Button(\"Submit\", variant=\"primary\")\n",
    "    output = gr.Textbox(lines=1, max_lines=30, label=\"Natural language translation and explanation:\")\n",
    "\n",
    "# Call chain_invoke function upon clicking the Submit button\n",
    "    button.click(chain_2b_invoke, textbox, outputs=output)\n",
    "\n",
    "demo.launch()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mtenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
